{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/osmarbraz/exemplos_Llama/blob/main/ExemplosGeracaoTexto_Llama2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78HE8FLsKN9Q"
      },
      "source": [
        "#Exemplo de Geração de textos usando Llama v2.0 7B 8bit usando Transformers by HuggingFace\n",
        "\n",
        "Exemplo de uso do modelo de linguagem grande Llama v2.0.\n",
        "- Analise da geração de textos\n",
        "- Prompts com textos emparelhados\n",
        "- Injentando padrões no prompt\n",
        "- Padrão Persona\n",
        "\n",
        "**Toda a execução ocorre no Google Colaboratory.**\n",
        "\n",
        "Pré-requisitos:\n",
        "- Lhama 2 não está acessível abertamente e requer solicitação  de acesso. Faça o cadastro no site do https://huggingface.co/join. Depois do login, gere um token de acesso no link https://huggingface.co/settings/tokens.\n",
        "- Configurar o notebook para usar GPU- Acesse o menu 'Ambiente de Execução -> Alterar o tipo do ambiente de execução -> Acelerador de hardware -> T4 GPU\n",
        "\n",
        "\n",
        "**Notebook de referência:**\n",
        "\n",
        "https://github.com/guardiaum/tutorial-sbbd2023/blob/main/Prompt_Engineering.ipynb\n",
        "\n",
        "\n",
        "**Lista dos modelos:**\n",
        "\n",
        "https://huggingface.co/models\n",
        "\n",
        "\n",
        "**Artigos referências:**\n",
        "\n",
        "https://dev.to/nithinibhandari1999/how-to-run-llama-2-on-your-local-computer-42g1\n",
        "\n",
        "\n",
        "**Link biblioteca Huggingface:**\n",
        "\n",
        "https://github.com/huggingface/transformers\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyxb5Px3p1-e"
      },
      "source": [
        "# 0 - Preparação do ambiente\n",
        "Preparação do ambiente para execução do exemplo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAPVtRXQqDim"
      },
      "source": [
        "## Tratamento de logs\n",
        "\n",
        "Método para tratamento dos logs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DcopxbGZqDip"
      },
      "outputs": [],
      "source": [
        "# Biblioteca de logging\n",
        "import logging\n",
        "\n",
        "# Formatando a mensagem de logging\n",
        "logging.basicConfig(format=\"%(asctime)s : %(levelname)s : %(message)s\", level=logging.INFO)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GjYtXcMnSAe"
      },
      "source": [
        "## Identificando o ambiente Colab\n",
        "\n",
        "Cria uma variável para identificar que o notebook está sendo executado no Google Colaboratory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YMiH0E3OnRa1"
      },
      "outputs": [],
      "source": [
        "# Biblioteca do sistema\n",
        "import sys\n",
        "\n",
        "# Retorna true ou false se estiver no Google Colaboratory\n",
        "IN_COLAB = \"google.colab\" in sys.modules"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Versão Python"
      ],
      "metadata": {
        "id": "B-xSroBtxPL2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Biblioteca do sistema\n",
        "import sys\n",
        "\n",
        "print(\"Versão Python:\", sys.version)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Xu2haQbxRTc",
        "outputId": "f0993634-19a9-4ac1-946c-ff8d8a626064"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Versão Python: 3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AoYjkIZfXgP"
      },
      "source": [
        "## Funções auxiliares"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "603LYIYKBmq5"
      },
      "source": [
        "Função auxiliar para formatar o tempo como `hh: mm: ss`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Guy6B4whsZFR"
      },
      "outputs": [],
      "source": [
        "# Import das bibliotecas.\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "def formataTempo(tempo):\n",
        "    \"\"\"\n",
        "      Pega a tempo em segundos e retorna uma string hh:mm:ss\n",
        "    \"\"\"\n",
        "    # Arredonda para o segundo mais próximo.\n",
        "    tempo_arredondado = int(round((tempo)))\n",
        "\n",
        "    # Formata como hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=tempo_arredondado))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pqa-7WXBAw8q"
      },
      "source": [
        "# 1 - Instalação das bibliotecas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zp0jVfo3QM3h"
      },
      "source": [
        "O bitsandbytes é um wrapper leve em torno de funções personalizadas CUDA, em particular otimizadores de 8 bits, multiplicação de matrizes (LLM.int8()) e funções de quantização. É uma dependência do accelerate.\n",
        "\n",
        "https://huggingface.co/blog/4bit-transformers-bitsandbytes\n",
        "\n",
        "https://pypi.org/project/bitsandbytes/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12GE2W3fQM_n",
        "outputId": "4b88bfc5-12f9-4f59-a43b-8704deadd49c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bitsandbytes==0.41.1 in /usr/local/lib/python3.10/dist-packages (0.41.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install bitsandbytes==0.41.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7wU6vuyAuPd"
      },
      "source": [
        "Accelerate é uma biblioteca que permite que o mesmo código PyTorch seja executado em qualquer configuração distribuída adicionando apenas quatro linhas de código. Otimiza as operações do PyTorch, especialmente na GPU.\n",
        "\n",
        "https://pypi.org/project/accelerate/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qTMID1rZAvx7",
        "outputId": "1294bf63-c5f0-4512-a856-173abadd1ef5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: accelerate==0.23.0 in /usr/local/lib/python3.10/dist-packages (0.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.23.0) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.23.0) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.23.0) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate==0.23.0) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.23.0) (2.1.0+cu118)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate==0.23.0) (0.18.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.23.0) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.23.0) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.23.0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.23.0) (3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.23.0) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.23.0) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.23.0) (2.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate==0.23.0) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate==0.23.0) (4.66.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.23.0) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.23.0) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.23.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.23.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.23.0) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate==0.23.0) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install accelerate==0.23.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCdqJCtQN52l"
      },
      "source": [
        "A Biblioteca A Biblioteca Transformers fornece APIs e ferramentas para baixar e treinar facilmente modelos pré-treinados de última geração para Processamento de linguagem natural, Visão computacional, Áudio, etc.\n",
        "\n",
        "Fornece uma maneira direta de usar modelos pré-treinados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1RfUN_KolV-f",
        "outputId": "5547c24f-01ae-4c09-ab2c-2ef8ee784c78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers==4.31.0 in /usr/local/lib/python3.10/dist-packages (4.31.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (3.12.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (0.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.31.0) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.31.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.31.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.31.0) (2023.7.22)\n"
          ]
        }
      ],
      "source": [
        "# Instala a última versão da biblioteca\n",
        "# !pip install transformers\n",
        "\n",
        "# A última versão do huggingface apresenta um problema:\n",
        "# UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1`\n",
        "# https://discuss.huggingface.co/t/help-with-llama-2-finetuning-setup/50035\n",
        "# Usar a versão 4.31.0\n",
        "\n",
        "# Instala uma versão específica da biblioteca\n",
        "!pip install -U transformers==4.31.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlrWrRP02tuZ"
      },
      "source": [
        "A Biblioteca huggingface-cli fornece vários comandos para interagir com o Hugging Face Hub a partir da linha de comando. Um desses comandos é o login, que permite aos usuários se autenticarem no Hub usando suas credenciais."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQxtD3Zk14ov",
        "outputId": "edce201c-d91b-4731-a658-9c700fac9a9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub==0.18.0 in /usr/local/lib/python3.10/dist-packages (0.18.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub==0.18.0) (3.12.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub==0.18.0) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub==0.18.0) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub==0.18.0) (4.66.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub==0.18.0) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub==0.18.0) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub==0.18.0) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub==0.18.0) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub==0.18.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub==0.18.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub==0.18.0) (2023.7.22)\n"
          ]
        }
      ],
      "source": [
        "!pip install huggingface_hub==0.18.0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Versão bibliotecas instaladas"
      ],
      "metadata": {
        "id": "9NdUAv1OyE7v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip freeze"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffCAEnsNyG4G",
        "outputId": "92cf115a-4f16-4b8a-bfa8-205b9c07a2da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "absl-py==1.4.0\n",
            "accelerate==0.24.0\n",
            "aiohttp==3.8.6\n",
            "aiosignal==1.3.1\n",
            "alabaster==0.7.13\n",
            "albumentations==1.3.1\n",
            "altair==4.2.2\n",
            "anyio==3.7.1\n",
            "appdirs==1.4.4\n",
            "argon2-cffi==23.1.0\n",
            "argon2-cffi-bindings==21.2.0\n",
            "array-record==0.5.0\n",
            "arviz==0.15.1\n",
            "astropy==5.3.4\n",
            "astunparse==1.6.3\n",
            "async-timeout==4.0.3\n",
            "atpublic==4.0\n",
            "attrs==23.1.0\n",
            "audioread==3.0.1\n",
            "autograd==1.6.2\n",
            "Babel==2.13.1\n",
            "backcall==0.2.0\n",
            "backoff==2.2.1\n",
            "bcrypt==4.0.1\n",
            "beautifulsoup4==4.11.2\n",
            "bidict==0.22.1\n",
            "bigframes==0.10.0\n",
            "bitsandbytes==0.41.1\n",
            "bleach==6.1.0\n",
            "blinker==1.4\n",
            "blis==0.7.11\n",
            "blosc2==2.0.0\n",
            "bokeh==3.2.2\n",
            "bqplot==0.12.42\n",
            "branca==0.6.0\n",
            "build==1.0.3\n",
            "CacheControl==0.13.1\n",
            "cachetools==5.3.2\n",
            "catalogue==2.0.10\n",
            "certifi==2023.7.22\n",
            "cffi==1.16.0\n",
            "chardet==5.2.0\n",
            "charset-normalizer==3.3.1\n",
            "chex==0.1.7\n",
            "chroma-hnswlib==0.7.3\n",
            "chromadb==0.4.15\n",
            "click==8.1.7\n",
            "click-plugins==1.1.1\n",
            "cligj==0.7.2\n",
            "cloudpickle==2.2.1\n",
            "cmake==3.27.7\n",
            "cmdstanpy==1.2.0\n",
            "colorcet==3.0.1\n",
            "coloredlogs==15.0.1\n",
            "colorlover==0.3.0\n",
            "colour==0.1.5\n",
            "community==1.0.0b1\n",
            "confection==0.1.3\n",
            "cons==0.4.6\n",
            "contextlib2==21.6.0\n",
            "contourpy==1.1.1\n",
            "cryptography==41.0.5\n",
            "cufflinks==0.17.3\n",
            "cupy-cuda11x==11.0.0\n",
            "cvxopt==1.3.2\n",
            "cvxpy==1.3.2\n",
            "cycler==0.12.1\n",
            "cymem==2.0.8\n",
            "Cython==3.0.4\n",
            "dask==2023.8.1\n",
            "dataclasses-json==0.6.1\n",
            "datascience==0.17.6\n",
            "db-dtypes==1.1.1\n",
            "dbus-python==1.2.18\n",
            "debugpy==1.6.6\n",
            "decorator==4.4.2\n",
            "defusedxml==0.7.1\n",
            "Deprecated==1.2.14\n",
            "diskcache==5.6.3\n",
            "distributed==2023.8.1\n",
            "distro==1.7.0\n",
            "dlib==19.24.2\n",
            "dm-tree==0.1.8\n",
            "docutils==0.18.1\n",
            "dopamine-rl==4.0.6\n",
            "duckdb==0.8.1\n",
            "earthengine-api==0.1.375\n",
            "easydict==1.11\n",
            "ecos==2.0.12\n",
            "editdistance==0.6.2\n",
            "eerepr==0.0.4\n",
            "en-core-web-sm @ https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.6.0/en_core_web_sm-3.6.0-py3-none-any.whl#sha256=83276fc78a70045627144786b52e1f2728ad5e29e5e43916ec37ea9c26a11212\n",
            "entrypoints==0.4\n",
            "et-xmlfile==1.1.0\n",
            "etils==1.5.2\n",
            "etuples==0.3.9\n",
            "exceptiongroup==1.1.3\n",
            "fastai==2.7.13\n",
            "fastapi==0.104.1\n",
            "fastcore==1.5.29\n",
            "fastdownload==0.0.7\n",
            "fastjsonschema==2.18.1\n",
            "fastprogress==1.0.3\n",
            "fastrlock==0.8.2\n",
            "filelock==3.12.4\n",
            "fiona==1.9.5\n",
            "firebase-admin==5.3.0\n",
            "Flask==2.2.5\n",
            "flatbuffers==23.5.26\n",
            "flax==0.7.4\n",
            "folium==0.14.0\n",
            "fonttools==4.43.1\n",
            "frozendict==2.3.8\n",
            "frozenlist==1.4.0\n",
            "fsspec==2023.6.0\n",
            "future==0.18.3\n",
            "gast==0.5.4\n",
            "gcsfs==2023.6.0\n",
            "GDAL==3.4.3\n",
            "gdown==4.6.6\n",
            "geemap==0.28.2\n",
            "gensim==4.3.2\n",
            "geocoder==1.38.1\n",
            "geographiclib==2.0\n",
            "geopandas==0.13.2\n",
            "geopy==2.3.0\n",
            "gin-config==0.5.0\n",
            "glob2==0.7\n",
            "google==2.0.3\n",
            "google-api-core==2.11.1\n",
            "google-api-python-client==2.84.0\n",
            "google-auth==2.17.3\n",
            "google-auth-httplib2==0.1.1\n",
            "google-auth-oauthlib==1.0.0\n",
            "google-cloud-bigquery==3.12.0\n",
            "google-cloud-bigquery-connection==1.12.1\n",
            "google-cloud-bigquery-storage==2.22.0\n",
            "google-cloud-core==2.3.3\n",
            "google-cloud-datastore==2.15.2\n",
            "google-cloud-firestore==2.11.1\n",
            "google-cloud-functions==1.13.3\n",
            "google-cloud-iam==2.12.2\n",
            "google-cloud-language==2.9.1\n",
            "google-cloud-resource-manager==1.10.4\n",
            "google-cloud-storage==2.8.0\n",
            "google-cloud-translate==3.11.3\n",
            "google-colab @ file:///colabtools/dist/google-colab-1.0.0.tar.gz#sha256=840b68ab91172cd01b4b48ca4b48fb1916e15b1e6cc74cdb3ad9bf40932339c5\n",
            "google-crc32c==1.5.0\n",
            "google-pasta==0.2.0\n",
            "google-resumable-media==2.6.0\n",
            "googleapis-common-protos==1.61.0\n",
            "googledrivedownloader==0.4\n",
            "graphviz==0.20.1\n",
            "greenlet==3.0.0\n",
            "grpc-google-iam-v1==0.12.6\n",
            "grpcio==1.59.0\n",
            "grpcio-status==1.48.2\n",
            "gspread==3.4.2\n",
            "gspread-dataframe==3.3.1\n",
            "gym==0.25.2\n",
            "gym-notices==0.0.8\n",
            "h11==0.14.0\n",
            "h5netcdf==1.2.0\n",
            "h5py==3.9.0\n",
            "holidays==0.35\n",
            "holoviews==1.17.1\n",
            "html5lib==1.1\n",
            "httpimport==1.3.1\n",
            "httplib2==0.22.0\n",
            "httptools==0.6.1\n",
            "huggingface-hub==0.18.0\n",
            "humanfriendly==10.0\n",
            "humanize==4.7.0\n",
            "hyperopt==0.2.7\n",
            "ibis-framework==6.2.0\n",
            "idna==3.4\n",
            "imageio==2.31.6\n",
            "imageio-ffmpeg==0.4.9\n",
            "imagesize==1.4.1\n",
            "imbalanced-learn==0.10.1\n",
            "imgaug==0.4.0\n",
            "importlib-metadata==6.8.0\n",
            "importlib-resources==6.1.0\n",
            "imutils==0.5.4\n",
            "inflect==7.0.0\n",
            "iniconfig==2.0.0\n",
            "install==1.3.5\n",
            "intel-openmp==2023.2.0\n",
            "ipyevents==2.0.2\n",
            "ipyfilechooser==0.6.0\n",
            "ipykernel==5.5.6\n",
            "ipyleaflet==0.17.4\n",
            "ipython==7.34.0\n",
            "ipython-genutils==0.2.0\n",
            "ipython-sql==0.5.0\n",
            "ipytree==0.2.2\n",
            "ipywidgets==7.7.1\n",
            "itsdangerous==2.1.2\n",
            "jax==0.4.16\n",
            "jaxlib @ https://storage.googleapis.com/jax-releases/cuda11/jaxlib-0.4.16+cuda11.cudnn86-cp310-cp310-manylinux2014_x86_64.whl#sha256=78b3a9acfda4bfaae8a1dc112995d56454020f5c02dba4d24c40c906332efd4a\n",
            "jeepney==0.7.1\n",
            "jieba==0.42.1\n",
            "Jinja2==3.1.2\n",
            "joblib==1.3.2\n",
            "jsonpatch==1.33\n",
            "jsonpickle==3.0.2\n",
            "jsonpointer==2.4\n",
            "jsonschema==4.19.1\n",
            "jsonschema-specifications==2023.7.1\n",
            "jupyter-client==6.1.12\n",
            "jupyter-console==6.1.0\n",
            "jupyter-server==1.24.0\n",
            "jupyter_core==5.4.0\n",
            "jupyterlab-pygments==0.2.2\n",
            "jupyterlab-widgets==3.0.9\n",
            "kaggle==1.5.16\n",
            "keras==2.14.0\n",
            "keyring==23.5.0\n",
            "kiwisolver==1.4.5\n",
            "kubernetes==28.1.0\n",
            "langchain==0.0.323\n",
            "langcodes==3.3.0\n",
            "langsmith==0.0.57\n",
            "launchpadlib==1.10.16\n",
            "lazr.restfulclient==0.14.4\n",
            "lazr.uri==1.0.6\n",
            "lazy_loader==0.3\n",
            "libclang==16.0.6\n",
            "librosa==0.10.1\n",
            "lida==0.0.10\n",
            "lightgbm==4.1.0\n",
            "linkify-it-py==2.0.2\n",
            "lit==15.0.7\n",
            "llmx==0.0.15a0\n",
            "llvmlite==0.39.1\n",
            "lmdb==1.4.1\n",
            "locket==1.0.0\n",
            "logical-unification==0.4.6\n",
            "lxml==4.9.3\n",
            "malloy==2023.1058\n",
            "Markdown==3.5\n",
            "markdown-it-py==3.0.0\n",
            "MarkupSafe==2.1.3\n",
            "marshmallow==3.20.1\n",
            "matplotlib==3.7.1\n",
            "matplotlib-inline==0.1.6\n",
            "matplotlib-venn==0.11.9\n",
            "mdit-py-plugins==0.4.0\n",
            "mdurl==0.1.2\n",
            "miniKanren==1.0.3\n",
            "missingno==0.5.2\n",
            "mistune==0.8.4\n",
            "mizani==0.9.3\n",
            "mkl==2023.2.0\n",
            "ml-dtypes==0.2.0\n",
            "mlxtend==0.22.0\n",
            "monotonic==1.6\n",
            "more-itertools==10.1.0\n",
            "moviepy==1.0.3\n",
            "mpmath==1.3.0\n",
            "msgpack==1.0.7\n",
            "multidict==6.0.4\n",
            "multipledispatch==1.0.0\n",
            "multitasking==0.0.11\n",
            "murmurhash==1.0.10\n",
            "music21==9.1.0\n",
            "mypy-extensions==1.0.0\n",
            "natsort==8.4.0\n",
            "nbclassic==1.0.0\n",
            "nbclient==0.8.0\n",
            "nbconvert==6.5.4\n",
            "nbformat==5.9.2\n",
            "nest-asyncio==1.5.8\n",
            "networkx==3.2\n",
            "nibabel==4.0.2\n",
            "nltk==3.8.1\n",
            "notebook==6.5.5\n",
            "notebook_shim==0.2.3\n",
            "numba==0.56.4\n",
            "numexpr==2.8.7\n",
            "numpy==1.23.5\n",
            "nvidia-cublas-cu12==12.1.3.1\n",
            "nvidia-cuda-cupti-cu12==12.1.105\n",
            "nvidia-cuda-nvrtc-cu12==12.1.105\n",
            "nvidia-cuda-runtime-cu12==12.1.105\n",
            "nvidia-cudnn-cu12==8.9.2.26\n",
            "nvidia-cufft-cu12==11.0.2.54\n",
            "nvidia-curand-cu12==10.3.2.106\n",
            "nvidia-cusolver-cu12==11.4.5.107\n",
            "nvidia-cusparse-cu12==12.1.0.106\n",
            "nvidia-nccl-cu12==2.18.1\n",
            "nvidia-nvjitlink-cu12==12.3.52\n",
            "nvidia-nvtx-cu12==12.1.105\n",
            "oauth2client==4.1.3\n",
            "oauthlib==3.2.2\n",
            "onnxruntime==1.16.1\n",
            "opencv-contrib-python==4.8.0.76\n",
            "opencv-python==4.8.0.76\n",
            "opencv-python-headless==4.8.1.78\n",
            "openpyxl==3.1.2\n",
            "opentelemetry-api==1.20.0\n",
            "opentelemetry-exporter-otlp-proto-common==1.20.0\n",
            "opentelemetry-exporter-otlp-proto-grpc==1.20.0\n",
            "opentelemetry-proto==1.20.0\n",
            "opentelemetry-sdk==1.20.0\n",
            "opentelemetry-semantic-conventions==0.41b0\n",
            "opt-einsum==3.3.0\n",
            "optax==0.1.7\n",
            "orbax-checkpoint==0.4.1\n",
            "osqp==0.6.2.post8\n",
            "overrides==7.4.0\n",
            "packaging==23.2\n",
            "pandas==1.5.3\n",
            "pandas-datareader==0.10.0\n",
            "pandas-gbq==0.17.9\n",
            "pandas-stubs==1.5.3.230304\n",
            "pandocfilters==1.5.0\n",
            "panel==1.3.0\n",
            "param==2.0.0\n",
            "parso==0.8.3\n",
            "parsy==2.1\n",
            "partd==1.4.1\n",
            "pathlib==1.0.1\n",
            "pathy==0.10.3\n",
            "patsy==0.5.3\n",
            "peewee==3.17.0\n",
            "pexpect==4.8.0\n",
            "pickleshare==0.7.5\n",
            "Pillow==9.4.0\n",
            "pip-tools==6.13.0\n",
            "platformdirs==3.11.0\n",
            "plotly==5.15.0\n",
            "plotnine==0.12.3\n",
            "pluggy==1.3.0\n",
            "polars==0.17.3\n",
            "pooch==1.8.0\n",
            "portpicker==1.5.2\n",
            "posthog==3.0.2\n",
            "prefetch-generator==1.0.3\n",
            "preshed==3.0.9\n",
            "prettytable==3.9.0\n",
            "proglog==0.1.10\n",
            "progressbar2==4.2.0\n",
            "prometheus-client==0.17.1\n",
            "promise==2.3\n",
            "prompt-toolkit==3.0.39\n",
            "prophet==1.1.5\n",
            "proto-plus==1.22.3\n",
            "protobuf==3.20.3\n",
            "psutil==5.9.5\n",
            "psycopg2==2.9.9\n",
            "ptyprocess==0.7.0\n",
            "pulsar-client==3.3.0\n",
            "py-cpuinfo==9.0.0\n",
            "py4j==0.10.9.7\n",
            "pyarrow==9.0.0\n",
            "pyasn1==0.5.0\n",
            "pyasn1-modules==0.3.0\n",
            "pycocotools==2.0.7\n",
            "pycparser==2.21\n",
            "pyct==0.5.0\n",
            "pydantic==1.10.13\n",
            "pydata-google-auth==1.8.2\n",
            "pydot==1.4.2\n",
            "pydot-ng==2.0.0\n",
            "pydotplus==2.0.2\n",
            "PyDrive==1.3.1\n",
            "PyDrive2==1.6.3\n",
            "pyerfa==2.0.1.1\n",
            "pygame==2.5.2\n",
            "Pygments==2.16.1\n",
            "PyGObject==3.42.1\n",
            "PyJWT==2.3.0\n",
            "pymc==5.7.2\n",
            "pymystem3==0.2.0\n",
            "PyOpenGL==3.1.7\n",
            "pyOpenSSL==23.2.0\n",
            "pyparsing==3.1.1\n",
            "pypdf==3.16.4\n",
            "pyperclip==1.8.2\n",
            "PyPika==0.48.9\n",
            "pyproj==3.6.1\n",
            "pyproject_hooks==1.0.0\n",
            "pyshp==2.3.1\n",
            "PySocks==1.7.1\n",
            "pytensor==2.14.2\n",
            "pytest==7.4.3\n",
            "python-apt==0.0.0\n",
            "python-box==7.1.1\n",
            "python-dateutil==2.8.2\n",
            "python-dotenv==1.0.0\n",
            "python-louvain==0.16\n",
            "python-slugify==8.0.1\n",
            "python-utils==3.8.1\n",
            "pytz==2023.3.post1\n",
            "pyviz_comms==3.0.0\n",
            "PyWavelets==1.4.1\n",
            "PyYAML==6.0.1\n",
            "pyzmq==23.2.1\n",
            "qdldl==0.1.7.post0\n",
            "qudida==0.0.4\n",
            "ratelim==0.1.6\n",
            "referencing==0.30.2\n",
            "regex==2023.6.3\n",
            "requests==2.31.0\n",
            "requests-oauthlib==1.3.1\n",
            "requirements-parser==0.5.0\n",
            "rich==13.6.0\n",
            "rpds-py==0.10.6\n",
            "rpy2==3.4.2\n",
            "rsa==4.9\n",
            "safetensors==0.4.0\n",
            "scikit-image==0.19.3\n",
            "scikit-learn==1.2.2\n",
            "scipy==1.11.3\n",
            "scooby==0.9.2\n",
            "scs==3.2.3\n",
            "seaborn==0.12.2\n",
            "SecretStorage==3.3.1\n",
            "Send2Trash==1.8.2\n",
            "sentence-transformers==2.2.2\n",
            "sentencepiece==0.1.99\n",
            "shapely==2.0.2\n",
            "six==1.16.0\n",
            "sklearn-pandas==2.2.0\n",
            "smart-open==6.4.0\n",
            "sniffio==1.3.0\n",
            "snowballstemmer==2.2.0\n",
            "sortedcontainers==2.4.0\n",
            "soundfile==0.12.1\n",
            "soupsieve==2.5\n",
            "soxr==0.3.7\n",
            "spacy==3.6.1\n",
            "spacy-legacy==3.0.12\n",
            "spacy-loggers==1.0.5\n",
            "Sphinx==5.0.2\n",
            "sphinxcontrib-applehelp==1.0.7\n",
            "sphinxcontrib-devhelp==1.0.5\n",
            "sphinxcontrib-htmlhelp==2.0.4\n",
            "sphinxcontrib-jsmath==1.0.1\n",
            "sphinxcontrib-qthelp==1.0.6\n",
            "sphinxcontrib-serializinghtml==1.1.9\n",
            "SQLAlchemy==2.0.22\n",
            "sqlglot==17.16.2\n",
            "sqlparse==0.4.4\n",
            "srsly==2.4.8\n",
            "stanio==0.3.0\n",
            "starlette==0.27.0\n",
            "statsmodels==0.14.0\n",
            "sympy==1.12\n",
            "tables==3.8.0\n",
            "tabulate==0.9.0\n",
            "tbb==2021.10.0\n",
            "tblib==3.0.0\n",
            "tenacity==8.2.3\n",
            "tensorboard==2.14.1\n",
            "tensorboard-data-server==0.7.2\n",
            "tensorflow==2.14.0\n",
            "tensorflow-datasets==4.9.3\n",
            "tensorflow-estimator==2.14.0\n",
            "tensorflow-gcs-config==2.14.0\n",
            "tensorflow-hub==0.15.0\n",
            "tensorflow-io-gcs-filesystem==0.34.0\n",
            "tensorflow-metadata==1.14.0\n",
            "tensorflow-probability==0.22.0\n",
            "tensorstore==0.1.45\n",
            "termcolor==2.3.0\n",
            "terminado==0.17.1\n",
            "text-unidecode==1.3\n",
            "textblob==0.17.1\n",
            "tf-slim==1.1.0\n",
            "thinc==8.1.12\n",
            "threadpoolctl==3.2.0\n",
            "tifffile==2023.9.26\n",
            "tiktoken==0.5.1\n",
            "tinycss2==1.2.1\n",
            "tokenizers==0.13.3\n",
            "toml==0.10.2\n",
            "tomli==2.0.1\n",
            "toolz==0.12.0\n",
            "torch==2.1.0\n",
            "torchaudio==2.0.2+cu118\n",
            "torchdata==0.6.1\n",
            "torchsummary==1.5.1\n",
            "torchtext==0.15.2+cpu\n",
            "torchvision==0.15.2+cu118\n",
            "tornado==6.3.2\n",
            "tqdm==4.66.1\n",
            "traitlets==5.7.1\n",
            "traittypes==0.2.1\n",
            "transformers==4.31.0\n",
            "triton==2.1.0\n",
            "tweepy==4.13.0\n",
            "typer==0.9.0\n",
            "types-pytz==2023.3.1.1\n",
            "types-setuptools==68.2.0.0\n",
            "typing-inspect==0.9.0\n",
            "typing_extensions==4.8.0\n",
            "tzlocal==5.2\n",
            "uc-micro-py==1.0.2\n",
            "uritemplate==4.1.1\n",
            "urllib3==1.26.18\n",
            "uvicorn==0.23.2\n",
            "uvloop==0.19.0\n",
            "vega-datasets==0.9.0\n",
            "wadllib==1.3.6\n",
            "wasabi==1.1.2\n",
            "watchfiles==0.21.0\n",
            "wcwidth==0.2.8\n",
            "webcolors==1.13\n",
            "webencodings==0.5.1\n",
            "websocket-client==1.6.4\n",
            "websockets==12.0\n",
            "Werkzeug==3.0.1\n",
            "widgetsnbextension==3.6.6\n",
            "wordcloud==1.9.2\n",
            "wrapt==1.14.1\n",
            "xarray==2023.7.0\n",
            "xarray-einstats==0.6.0\n",
            "xformers==0.0.22.post7\n",
            "xgboost==2.0.1\n",
            "xlrd==2.0.1\n",
            "xxhash==3.4.1\n",
            "xyzservices==2023.10.0\n",
            "yarl==1.9.2\n",
            "yellowbrick==1.5\n",
            "yfinance==0.2.31\n",
            "zict==3.0.0\n",
            "zipp==3.17.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bcpd9t9PpkrX"
      },
      "source": [
        "# 2 - Carregando o LLM\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFRSYoCArrQ-"
      },
      "source": [
        "## 2.1 - Login no huggingface\n",
        "\n",
        "- Lhama 2 não está acessível abertamente e requer solicitação  de acesso. Faça o cadastro no site do https://huggingface.co/join. Depois do login, gere um token de acesso no link https://huggingface.co/settings/tokens.\n",
        "\n",
        "Insira o token quando solicitado e depois digite Y para adicionar as credenciais."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0bkqIoNU18UH"
      },
      "outputs": [],
      "source": [
        "# !huggingface-cli login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACJuj9wB9kjZ"
      },
      "source": [
        "Se o seu notebook não for público e não desejar incluir o token de acesso toda vez que for executar o notebook preencha o método save_token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lRVr7uqp9Ubk"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub.hf_api import HfFolder\n",
        "\n",
        "ACCESS_TOKEN  = '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'\n",
        "\n",
        "HfFolder.save_token(ACCESS_TOKEN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIzrrJLw9oQd"
      },
      "source": [
        "Mostrando o usuário conectado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xLrSstlxR_kq",
        "outputId": "ef15d065-616b-4c16-e9fb-db308bffec88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "osmarbraz\n"
          ]
        }
      ],
      "source": [
        "!huggingface-cli whoami"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niwUEmYM6kjG"
      },
      "source": [
        "## 2.2 - Nome do modelo de linguagem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBOzL86X6kjM"
      },
      "source": [
        "Define o nome do modelo a ser carregado\n",
        "Lista dos modelos:\n",
        "  - https://huggingface.co/meta-llama/Llama-2-7b-hf\n",
        "  - https://huggingface.co/meta-llama/Llama-2-7b-chat-hf\n",
        "  - https://huggingface.co/meta-llama/Llama-2-13b-hf\n",
        "  - https://huggingface.co/meta-llama/Llama-2-13b-chat-hf\n",
        "  - https://huggingface.co/meta-llama/Llama-2-70b-hf\n",
        "  - https://huggingface.co/meta-llama/Llama-2-70b-chat-hf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-zOnSymM6kjM"
      },
      "outputs": [],
      "source": [
        "#nome_modelo = \"meta-llama/Llama-2-7b-hf\"\n",
        "nome_modelo = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "\n",
        "# Não carrega por falta de memória no google colab\n",
        "#nome_modelo = \"meta-llama/Llama-2-13b-hf\"\n",
        "#nome_modelo = \"meta-llama/Llama-2-13b-chat-hf\"\n",
        "\n",
        "# Não carrega por falta de memória e espaço em disco no google colab\n",
        "#nome_modelo = \"meta-llama/Llama-2-70b-hf\"\n",
        "#nome_modelo = \"meta-llama/Llama-2-70b-chat-hf\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzWcQNSORrYC"
      },
      "source": [
        "## 2.3 - Carrega o tokenizador\n",
        "\n",
        "Carregando o **tokenizador** da comunidade."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MlSM1VufRw5B",
        "outputId": "4dae0dbd-737b-421e-a758-9611aa0467da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Carregando o tokenizador meta-llama/Llama-2-7b-chat-hf da comunidade...\n"
          ]
        }
      ],
      "source": [
        "# Importando as bibliotecas do Tokenizador\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Carregando o Tokenizador da comunidade\n",
        "print('Carregando o tokenizador ' + nome_modelo + ' da comunidade...')\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(nome_modelo)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tamanho do vocabulário"
      ],
      "metadata": {
        "id": "pNhZxBfM0LEe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(tokenizer))"
      ],
      "metadata": {
        "id": "IzgbIOUI0LEf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6ebf4a4-8997-4754-fec9-9d40d365fc49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhNMEhN9BHuc"
      },
      "source": [
        "## 2.4 - Carregando o LLM\n",
        "\n",
        "Carregando o **LLM** da comunidade HuggingFace.\n",
        "\n",
        "Parametrização do from_pretrained\n",
        "https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Carregamento LLama 2 com 4 bits"
      ],
      "metadata": {
        "id": "oj6_jlk35AUI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Importando as bibliotecas do Modelo\n",
        "# from transformers import BitsAndBytesConfig, AutoModelForCausalLM\n",
        "# import torch\n",
        "# import time\n",
        "\n",
        "# # Guarda o tempo de início do carregamento do modelo\n",
        "# tempo_inicio = time.time()\n",
        "\n",
        "# # Carregando o Modelo da comunidade\n",
        "# print('Carregando o modelo ' + nome_modelo + ' da comunidade...')\n",
        "\n",
        "# # BitsAndBytes é um framework com funções customizadas para\n",
        "# # otimização com precisão 8-bit, multiplicações de matrizes e funções de quantização\n",
        "# quantization_config = BitsAndBytesConfig(\n",
        "#    load_in_4bit=True, # Habilita a quantização de 4 bits para comprimir o modelo\n",
        "#    bnb_4bit_quant_type=\"nf4\", # Define o tipo de dados de quantização nas camadas (`fp4` e `nf4`).\n",
        "#    bnb_4bit_use_double_quant=True, # Quantização aninhada, onde as constantes de quantização da primeira quantização são quantizadas novamente.\n",
        "#    bnb_4bit_compute_dtype=torch.bfloat16 # # Os gradientes dos pesos são computados em 16-bit. Define o tipo computacional que pode ser diferente do tempo de entrada. Por exemplo, as entradas podem ser fp32, mas a computação pode ser definida como bf16 para acelerações.\n",
        "# )\n",
        "\n",
        "# # Carrega o modelo\n",
        "# model = AutoModelForCausalLM.from_pretrained(nome_modelo,\n",
        "#                                              #torch_dtype=torch.float16, #default\n",
        "#                                              trust_remote_code=True, # Carrega de um repositório confiável\n",
        "#                                              quantization_config=quantization_config,\n",
        "#                                              device_map=\"auto\"\n",
        "#                                              )\n",
        "\n",
        "# # Coloca o modelo e modo avaliação\n",
        "# model.eval()\n",
        "\n",
        "# # Aumentar a velocidade\n",
        "# # https://huggingface.co/docs/transformers/main/perf_torch_compile\n",
        "# model = torch.compile(model)\n",
        "\n",
        "# print(\"Tempo de carregamento do modelo LLM:  {:} (h:mm:ss)\".format(formataTempo(time.time() - tempo_inicio)))"
      ],
      "metadata": {
        "id": "5Kx5Ed64YlV-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Carregamento LLama 2 com 8 bits"
      ],
      "metadata": {
        "id": "doIqirI05Dos"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importando as bibliotecas do Modelo\n",
        "from transformers import BitsAndBytesConfig, AutoModelForCausalLM\n",
        "import torch\n",
        "import time\n",
        "\n",
        "# Guarda o tempo de início do carregamento do modelo\n",
        "tempo_inicio = time.time()\n",
        "\n",
        "# Carregando o Modelo da comunidade\n",
        "print('Carregando o modelo ' + nome_modelo + ' da comunidade...')\n",
        "\n",
        "# BitsAndBytes é um framework com funções customizadas para\n",
        "# otimização com precisão 8-bit, multiplicações de matrizes e funções de quantização\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "   load_in_8bit=True, # Habilita a quantização de 8 bits\n",
        ")\n",
        "\n",
        "# Carrega o modelo\n",
        "model = AutoModelForCausalLM.from_pretrained(nome_modelo,\n",
        "                                             #torch_dtype=torch.float16, #default\n",
        "                                             trust_remote_code=True, # Carrega de um repositório confiável\n",
        "                                             quantization_config=quantization_config,\n",
        "                                             device_map=\"auto\"\n",
        "                                             )\n",
        "\n",
        "# Coloca o modelo e modo avaliação\n",
        "model.eval()\n",
        "\n",
        "# Aumentar a velocidade\n",
        "# https://huggingface.co/docs/transformers/main/perf_torch_compile\n",
        "model = torch.compile(model)\n",
        "\n",
        "print(\"Tempo de carregamento do modelo LLM:  {:} (h:mm:ss)\".format(formataTempo(time.time() - tempo_inicio)))"
      ],
      "metadata": {
        "id": "WTFnuVQ3R0gp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "ad52ae2f8bba45d3a35491d91b9b3d93",
            "aaf58664bf3a438bb9227addd8e8249e",
            "9702d12eb1d542ea9462e0cb086c34b9",
            "4f8858e2b96d40ac9a89dbb119b28d86",
            "c3e3b9aee3504401a5d081e2356d73f2",
            "27b2ae11423444468e846a9165d4c9bf",
            "87047b76d0404185835eaece9d829f13",
            "a0c8954c073e439fb3734c8b92eacb17",
            "91bfab58d814407a9db0cd794fcd4ed8",
            "962a4abd92e64981a7a722f9679e2aa7",
            "dd40ddb78b3347b3b49942d003d83d54"
          ]
        },
        "outputId": "a50ce494-5324-4dfb-fda1-05cc7ee9185d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Carregando o modelo meta-llama/Llama-2-7b-chat-hf da comunidade...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ad52ae2f8bba45d3a35491d91b9b3d93"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tempo de carregamento do modelo LLM:  0:01:19 (h:mm:ss)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E11NM4T6pmpP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8375359f-40a8-4582-9979-c1903ca8a430"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OptimizedModule(\n",
            "  (_orig_mod): LlamaForCausalLM(\n",
            "    (model): LlamaModel(\n",
            "      (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
            "      (layers): ModuleList(\n",
            "        (0-31): 32 x LlamaDecoderLayer(\n",
            "          (self_attn): LlamaAttention(\n",
            "            (q_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
            "            (k_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
            "            (v_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
            "            (o_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
            "            (rotary_emb): LlamaRotaryEmbedding()\n",
            "          )\n",
            "          (mlp): LlamaMLP(\n",
            "            (gate_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
            "            (up_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
            "            (down_proj): Linear8bitLt(in_features=11008, out_features=4096, bias=False)\n",
            "            (act_fn): SiLUActivation()\n",
            "          )\n",
            "          (input_layernorm): LlamaRMSNorm()\n",
            "          (post_attention_layernorm): LlamaRMSNorm()\n",
            "        )\n",
            "      )\n",
            "      (norm): LlamaRMSNorm()\n",
            "    )\n",
            "    (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vXgoG2ZvuHFI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65f927af-97e4-474b-f581-58fe0c470604"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LlamaConfig {\n",
            "  \"_name_or_path\": \"meta-llama/Llama-2-7b-chat-hf\",\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 11008,\n",
            "  \"max_position_embeddings\": 4096,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 32,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"quantization_config\": {\n",
            "    \"bnb_4bit_compute_dtype\": \"float32\",\n",
            "    \"bnb_4bit_quant_type\": \"fp4\",\n",
            "    \"bnb_4bit_use_double_quant\": false,\n",
            "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
            "    \"llm_int8_has_fp16_weight\": false,\n",
            "    \"llm_int8_skip_modules\": null,\n",
            "    \"llm_int8_threshold\": 6.0,\n",
            "    \"load_in_4bit\": false,\n",
            "    \"load_in_8bit\": true\n",
            "  },\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float16\",\n",
            "  \"transformers_version\": \"4.31.0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(model.config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ysqp5fuyRWc4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f074e07-9ef8-438e-afb2-e1d7e97cae4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4096\n"
          ]
        }
      ],
      "source": [
        "print(model.config.max_position_embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tamanho do vocabulário"
      ],
      "metadata": {
        "id": "mpGMYgt6zWtX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.config.vocab_size)"
      ],
      "metadata": {
        "id": "ZT7nQq3Q0ALQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab10004a-83bd-4c8d-b6aa-51621d352bb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.5 - Configuração da geração de texto"
      ],
      "metadata": {
        "id": "Fjs8uvajG5gc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import das bibliotecas\n",
        "from transformers import GenerationConfig\n",
        "\n",
        "# Instância as configurações do modelo\n",
        "generation_config = GenerationConfig.from_pretrained(nome_modelo)\n",
        "\n",
        "print(\"GenerationConfig antes:\\n\",generation_config)\n",
        "generation_config.max_new_tokens = 512 #Preenche até um comprimento máximo especificado com o argumento max_length ou até o comprimento de entrada máximo aceitável para o modelo se esse argumento não for fornecido.\n",
        "#generation_config.max_length = 4096 # (Default 4096)\n",
        "generation_config.temperature = 0.1 # (Default 0.6) A temperatura é um parâmetro que controla a aleatoriedade da saída do LLM. Uma temperatura mais alta resultará em um texto mais criativo e imaginativo, enquanto uma temperatura mais baixa resultará em um texto mais preciso e factual.\n",
        "#generation_config.top_k = 3  # Top-k diz ao modelo para escolher o próximo token entre os 'k' tokens principais de sua lista, classificados por probabilidade.\n",
        "#generation_config.top_p = 0.9 # (Default 0.9) Top-p é mais dinâmico que top-k e é frequentemente usado para excluir resultados com probabilidades mais baixas. Portanto, se você definir p como 0,75, excluirá os 25% inferiores dos resultados prováveis.\n",
        "#generation_config.do_sample = True # (Default True) Se definido como True, este parâmetro permite estratégias de decodificação como amostragem multinomial, amostragem multinomial de busca de feixe, amostragem Top-K e amostragem Top-p. Todas essas estratégias selecionam o próximo token da distribuição de probabilidade em todo o vocabulário com vários ajustes específicos da estratégia.\n",
        "#generation_config.repetition_penalty = 1.20 # Penaliza a repetição e visa evitar frases que se repetem sem nada de realmente interessante.\n",
        "#generation_config.num_return_sequences=1, # Retorna uma única sentença da saída.\n",
        "print(\"GenerationConfig depois:\\n\",generation_config)"
      ],
      "metadata": {
        "id": "LmIPtO-RG-_b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf7be1d4-897e-4146-8ce5-cd3859d17414"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GenerationConfig antes:\n",
            " GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"do_sample\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"max_length\": 4096,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"temperature\": 0.6,\n",
            "  \"top_p\": 0.9,\n",
            "  \"transformers_version\": \"4.31.0\"\n",
            "}\n",
            "\n",
            "GenerationConfig depois:\n",
            " GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"do_sample\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"max_length\": 4096,\n",
            "  \"max_new_tokens\": 512,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"temperature\": 0.1,\n",
            "  \"top_p\": 0.9,\n",
            "  \"transformers_version\": \"4.31.0\"\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qezcBkxnEdR"
      },
      "source": [
        "# 3 - Analisando a geração de textos\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4Pd6-h0YD8U"
      },
      "source": [
        "## 3.1 - Geração de texto\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define o documento"
      ],
      "metadata": {
        "id": "6xxMMYLV1Dzm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8QP-2tC8YOFW",
        "outputId": "0c663e5d-b218-4af6-fdb1-8ef47959fb8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 <s>\n",
            "1 ▁Como\n",
            "2 ▁emp\n",
            "3 il\n",
            "4 har\n",
            "5 ▁elementos\n",
            "6 ▁em\n",
            "7 ▁uma\n",
            "8 ▁pil\n",
            "9 ha\n",
            "10 ?\n"
          ]
        }
      ],
      "source": [
        "# Define o documento base\n",
        "documento = \"Como empilhar elementos em uma pilha?\"\n",
        "#documento = \"How to push elements in a stack\"\n",
        "#documento = \"O comando SQL para extrair todos os usuários cujo nome começa com A é:\"\n",
        "#documento = \"Bom dia professor, tudo bem ?\"\n",
        "# documento = \"The SQL command to extract all the users whose name starts with A is:\"\n",
        "#documento = \"How to push elements in a stack\"\n",
        "#documento = \"Write code for finding the prime number in python ?\"\n",
        "# documento = \"Escrever código para encontrar o número primo em python?\"\n",
        "\n",
        "# Prepara o prompt para enviar ao modelo realizando sua tokenização\n",
        "# Se pt for especificado, ele retornará tensores em vez de lista de inteiros python e tokenizará os documentos\n",
        "input = tokenizer(documento, return_tensors=\"pt\")\n",
        "\n",
        "# Mostra os tokens com seus índices\n",
        "i = 0\n",
        "for tup in input.input_ids[0]:\n",
        "    # print(tup.item())\n",
        "    print(\"{} {}\".format(i, tokenizer.convert_ids_to_tokens(tup.item())))\n",
        "    i= i + 1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configura e envia o texto ao LLM"
      ],
      "metadata": {
        "id": "LTpS6fNC1F_m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6sMpJpeewC-i"
      },
      "outputs": [],
      "source": [
        "# Import das bibliotecas\n",
        "from transformers import GenerationConfig\n",
        "import torch\n",
        "import gc\n",
        "\n",
        "# Conecta a entrada prerada ao mesmo dispositivo de computação do modelo\n",
        "input_ids = input[\"input_ids\"].to(model.device)\n",
        "\n",
        "# Envia a prompt preparado ao modelo\n",
        "outputs = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        generation_config=generation_config, # Passa as configurações da geração de texto para o modelo\n",
        "        # https://huggingface.co/docs/transformers/v4.34.1/en/main_classes/output#transformers.utils.ModelOutput\n",
        "        return_dict_in_generate=True, # A geração retorna um dicionário com 'last_hidden_state' o estado ocultos da última camada do modelo, 'hidden_states' estados ocultos do modelo na saída de cada camada mais as saídas dos embeddings iniciais opcionais  e 'attention' com os psos de atenção após o softmax de atenção, usado para calcular a média ponderada nas cabeças de autoatenção.\n",
        "        output_scores=True, # Retorna as pontuações de previsão do modelo.\n",
        "        max_new_tokens=256 # O número máximo de tokens a serem gerados, ignorando o número de tokens no prompt.\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edTQHEauxnQQ",
        "outputId": "f5b76fb3-0a24-448e-cfcb-b96542244e7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        }
      ],
      "source": [
        "print(len(outputs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u715aSgPU7KM",
        "outputId": "d3ac6574-8e85-416d-a7e0-170e99812c97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SampleDecoderOnlyOutput(sequences=tensor([[    1, 17295,  3710,   309,  8222, 29290,   953,  3672,  8230,  2350,\n",
            "         29973,    13,    13, 29909,  6578,  7919,  1922,   429, 13141,   316,\n",
            "          1986,  3710,   309,  8222, 29290,   953,  3672,  8230,  2350, 29901,\n",
            "            13,    13, 29896, 29889, 16760,   346,   419,  1922,  1543, 29877,\n",
            "           289,  1569,  1417, 29892,  1986,  1922,  6668,  1111,   316,  1754,\n",
            "          3055, 29889,    13, 29906, 29889,  2087,   293,  1421,  1922,  1543,\n",
            "         29877,   594, 18394,  5017,  6668,  1111,   316,  1754,  3055, 29892,\n",
            "          1986,  1922,   282,  8710,  6102,   316, 11915, 29889,    13, 29941,\n",
            "         29889,  2866, 14150,   594, 15353,  1743, 29290,  5017,  6668,  1111,\n",
            "           316,  1754,  3055, 29892,  1986, 21950,  8939,   562,   743,   316,\n",
            "         11915,  2123, 21950, 13413,   359, 29889,    13, 29946, 29889, 18410,\n",
            "         22781, 29899,   344,   316,   712,  9747,  1543, 29877,  4404,  1764,\n",
            "         15522,   953,  5521,   912,   694,   304,  1129,   437,  6668,  1111,\n",
            "           316,  1754,  3055, 12971,   316,   594, 15353,   279,  3503, 29290,\n",
            "         29889,    13, 29945, 29889,  2866, 14150,  3710,   309, 29882,  1743,\n",
            "         29290, 16659,   712,   263,  8230,  2350,  3006,  2350,   263,  5272,\n",
            "          2002,   553, 10337,  1114, 29889,    13,    13, 30062, 13483,  9336,\n",
            "          1182,   279,   712,   263,  8230,  2350, 28542,   724,  6787, 28815,\n",
            "           316,  5954,  2377,  2002,   321,  7919,   955, 29892,  1702,  3415,\n",
            "          3673,   712, 25192,   553,  5444,   802,  2123,   712,  2897, 29290,\n",
            "           409,   553,  1646,   403, 29889,   838,  2249,   766,   578, 29892,\n",
            "           904, 13483, 28336, 29290,   712,   409, 29926,   314, 19967,   339,\n",
            "          2255,  1702,   263,  6787,  2340,  1146,  8230,  2350,   321,   712,\n",
            "          3119,   314,   480,   637,   279,   263,   274, 21899,  3001,  1146,\n",
            "          8230,  2350, 29889,    13,    13, 29909,  6578,   707,  1368,  3093,\n",
            "           398,   294,   270,  5070,   594, 15353,  1759,  1702,  3710,   309,\n",
            "          8222, 29290,   953,  3672,  8230,  2350, 29901]], device='cuda:0'), scores=(tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0')), attentions=None, hidden_states=None)\n"
          ]
        }
      ],
      "source": [
        "print(outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzHNNOVqwrqn"
      },
      "source": [
        "Decodificação\n",
        "\n",
        "Nossa etapa de geração gera uma matriz de tokens em vez de palavras. Para converter esses tokens em palavras, precisamos realizar sua decodificação."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4lNj1xhA0bb",
        "outputId": "0ef2d95c-773b-42d9-c28d-e44091fbed9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Como empilhar elementos em uma pilha?\n",
            "\n",
            "Aqui está um exemplo de como empilhar elementos em uma pilha:\n",
            "\n",
            "1. Comece com um elemento básico, como um bloco de madeira.\n",
            "2. Adicione um elemento adicional ao bloco de madeira, como um pedaço de metal.\n",
            "3. Continue adicionando elementos ao bloco de madeira, como outros pedacés de metal ou outros objetos.\n",
            "4. Certifique-se de que cada elemento esteja bem embalado no topo do bloco de madeira antes de adicionar mais elementos.\n",
            "5. Continue empilhando elementos até que a pilha tenha a altura desejada.\n",
            "\n",
            "É importante lembrar que a pilha deve ser construída de forma segura e estável, para evitar que ela desfaque ou que os elementos se desbarate. Além disso, é importante usar elementos que sejam adequados para a construção da pilha e que possam suportar a carga total da pilha.\n",
            "\n",
            "Aqui estão algumas dicas adicionais para empilhar elementos em uma pilha:\n"
          ]
        }
      ],
      "source": [
        "# Mostra o resultado\n",
        "for s in outputs.sequences:\n",
        "  # Decodifica a saída\n",
        "  # skip_special_tokens=True retira os tokens especiais da saída da decodificação\n",
        "  output = tokenizer.decode(s, skip_special_tokens=True)\n",
        "  print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Op0fqNsowdTu",
        "outputId": "82a11f20-78b6-40cc-f182-41ac6e05b634"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Como empilhar elementos em uma pilha?\n",
            "\n",
            "Aqui está um exemplo de como empilhar elementos em uma pilha:\n",
            "\n",
            "1. Comece com um elemento básico, como um bloco de madeira.\n",
            "2. Adicione um elemento adicional ao bloco de madeira, como um pedaço de metal.\n",
            "3. Continue adicionando elementos ao bloco de madeira, como outros pedacés de metal ou outros objetos.\n",
            "4. Certifique-se de que cada elemento esteja bem embalado no topo do bloco de madeira antes de adicionar mais elementos.\n",
            "5. Continue empilhando elementos até que a pilha tenha a altura desejada.\n",
            "\n",
            "É importante lembrar que a pilha deve ser construída de forma segura e estável, para evitar que ela desfaque ou que os elementos se desbarate. Além disso, é importante usar elementos que sejam adequados para a construção da pilha e que possam suportar a carga total da pilha.\n",
            "\n",
            "Aqui estão algumas dicas adicionais para empilhar elementos em uma pilha:\n"
          ]
        }
      ],
      "source": [
        "# Mostra o resultado\n",
        "# skip_special_tokens=True retira os tokens especiais da saída da decodificação\n",
        "print(tokenizer.decode(outputs.sequences[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 - Geração de texto com prompt\n",
        "\n",
        "https://medium.com/@princekrampah/langchain-building-language-model-applications-c54cfe7219cb\n",
        "\n",
        "https://github.com/awesome-chatgpt-prompts/awesome-chatgpt-prompts-github"
      ],
      "metadata": {
        "id": "u7wyL25yzWVX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define o documento"
      ],
      "metadata": {
        "id": "K-zBWmok17CS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define o documento base\n",
        "documento = \"Como empilhar elementos em uma pilha?\""
      ],
      "metadata": {
        "id": "PJL1nANczg--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cria o prompt substituindo o parâmetro {documento} pelo conteúdo da variável documento."
      ],
      "metadata": {
        "id": "h71Vw3CT0Add"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"\"\"Pergunta: {documento}\n",
        "Resposta: Responda passo a passo.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "KgK1twC3zVO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configura e envia o prompt ao LLM"
      ],
      "metadata": {
        "id": "bsIFCD4q0lZ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import das bibliotecas\n",
        "from transformers import GenerationConfig\n",
        "import torch\n",
        "import gc\n",
        "\n",
        "# Conecta a entrada prerada ao mesmo dispositivo de computação do modelo\n",
        "input_ids = input[\"input_ids\"].to(model.device)\n",
        "\n",
        "# Prepara o prompt para enviar ao modelo realizando sua tokenização\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "# Conecta a entrada prerada ao mesmo dispositivo de computação do modelo\n",
        "input_ids = inputs[\"input_ids\"].to(model.device)\n",
        "\n",
        "# Envia a prompt preparado ao modelo\n",
        "output = model.generate(\n",
        "    input_ids=input_ids,\n",
        "    generation_config=generation_config,\n",
        "    return_dict_in_generate=True,\n",
        "    output_scores=True,\n",
        "    #max_new_tokens=512\n",
        ")"
      ],
      "metadata": {
        "id": "2IjNj2jS0k1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decodificação\n",
        "\n",
        "Nossa etapa de geração gera uma matriz de tokens em vez de palavras. Para converter esses tokens em palavras, precisamos realizar sua decodificação."
      ],
      "metadata": {
        "id": "G9G2r2Pv0wBR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mostra o resultado\n",
        "for s in outputs.sequences:\n",
        "  # Decodifica a saída\n",
        "  # skip_special_tokens=True retira os tokens especiais da saída da decodificação\n",
        "  output = tokenizer.decode(s, skip_special_tokens=True)\n",
        "  print(output)"
      ],
      "metadata": {
        "id": "0G7KRoA10xmO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bc74105-0a1a-446c-a0cc-0cbe3e08debc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Como empilhar elementos em uma pilha?\n",
            "\n",
            "Aqui está um exemplo de como empilhar elementos em uma pilha:\n",
            "\n",
            "1. Comece com um elemento básico, como um bloco de madeira.\n",
            "2. Adicione um elemento adicional ao bloco de madeira, como um pedaço de metal.\n",
            "3. Continue adicionando elementos ao bloco de madeira, como outros pedacés de metal ou outros objetos.\n",
            "4. Certifique-se de que cada elemento esteja bem embalado no topo do bloco de madeira antes de adicionar mais elementos.\n",
            "5. Continue empilhando elementos até que a pilha tenha a altura desejada.\n",
            "\n",
            "É importante lembrar que a pilha deve ser construída de forma segura e estável, para evitar que ela desfaque ou que os elementos se desbarate. Além disso, é importante usar elementos que sejam adequados para a construção da pilha e que possam suportar a carga total da pilha.\n",
            "\n",
            "Aqui estão algumas dicas adicionais para empilhar elementos em uma pilha:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Fkf18je3hCw"
      },
      "source": [
        "# 4 - Exemplo de prompts analisando textos emparelhados\n",
        "\n",
        "https://github.com/awesome-chatgpt-prompts/awesome-chatgpt-prompts-github\n",
        "\n",
        "Repositório de pompts: https://github.com/awesome-chatgpt-prompts/awesome-chatgpt-prompts-github\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5xMLwbfA3lHg"
      },
      "outputs": [],
      "source": [
        "def gerar_prompt(texto, entrada=None):\n",
        "    if entrada:\n",
        "        return f\"\"\"Abaixo está uma instrução que descreve uma tarefa, emparelhada com uma entrada que fornece mais contexto. Escreva uma resposta que conclua adequadamente a solicitação.\n",
        "\n",
        "### Instruções:\n",
        "{texto}\n",
        "\n",
        "### Entrada:\n",
        "{entrada}\n",
        "\n",
        "### Resposta:\"\"\"\n",
        "    else:\n",
        "        return f\"\"\"Abaixo está uma instrução que descreve uma tarefa. Escreva uma resposta que conclua adequadamente a solicitação.\n",
        "\n",
        "### Instruções:\n",
        "{texto}\n",
        "\n",
        "### Resposta:\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vvhp6BTz31sz"
      },
      "outputs": [],
      "source": [
        "# Import das bibliotecas\n",
        "from transformers import GenerationConfig\n",
        "import torch\n",
        "import gc\n",
        "\n",
        "def avaliarTexto(texto, entrada=None):\n",
        "    # Recupera o prompt\n",
        "    prompt = gerar_prompt(texto, entrada)\n",
        "\n",
        "    # Prepara o prompt para enviar ao modelo realizando sua tokenização\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    # Conecta a entrada prerada ao mesmo dispositivo de computação do modelo\n",
        "    input_ids = inputs[\"input_ids\"].to(model.device)\n",
        "\n",
        "    # Envia a prompt preparado ao modelo\n",
        "    output = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        generation_config=generation_config,\n",
        "        return_dict_in_generate=True,\n",
        "        output_scores=True,\n",
        "        #max_new_tokens=512\n",
        "    )\n",
        "\n",
        "    # Mostra a saída\n",
        "    for s in output.sequences:\n",
        "        # Decodifica a saída\n",
        "        output = tokenizer.decode(s, skip_special_tokens=True)\n",
        "        # Recupera a saída da resposta\n",
        "        saida = output.split(\"### Resposta:\")[1].strip()\n",
        "        print(\"Resposta: \\n\" + saida)\n",
        "\n",
        "    # Liberar memória\n",
        "    del input_ids\n",
        "    del output\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPqNyPAXV2aH"
      },
      "source": [
        "## 4.1 - Tarefa simples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lsuoM33I35aU",
        "outputId": "b5f0bc8e-40a9-4557-bb1d-45c7b51cc030"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resposta: \n",
            "Algoritmos são sequências de estágios de resolução de problemas que são usados para resolver problemas complexos em computação. Eles são usados em uma variedade de aplicações, como processamento de linguagem natural, visão computacional, inteligência artificial e análise de dados. Algoritmos podem ser clasificados em diferentes categorias, como algoritmos de busca, algoritmos de processamento de linguagem, algoritmos de aprendizado de máquina e algoritmos de análise de dados. Cada tipo de algoritmo tem suas próprias características e é usado para resolver problemas específicos. Alguns dos algoritmos mais comuns incluem o algoritmo de busca de Dijkstra, o algoritmo de processamento de linguagem natural de NLTK, o algoritmo de aprendizado de máquina de scikit-learn e o algoritmo de análise de dados de pandas.\n"
          ]
        }
      ],
      "source": [
        "texto = 'Me fale sobre algoritmos.'\n",
        "avaliarTexto(texto)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nx3CH6GfV8C1"
      },
      "source": [
        "## 4.2 - Tarefa com entrada"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6GtAPgns4Qxu",
        "outputId": "c1f3f4e4-bbca-45d5-c9d4-8284f46fa907"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resposta: \n",
            "A massa molar de CaCl2 é de aproximadamente 105,5 g/mol.\n",
            "\n",
            "Raisei! 😃\n"
          ]
        }
      ],
      "source": [
        "texto = 'Dada a fórmula química, calcule a massa molar.'\n",
        "\n",
        "entrada = 'CaCl2'\n",
        "\n",
        "avaliarTexto(texto, entrada)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPcD-rCP4cUy",
        "outputId": "5b3c00bd-e991-4f61-ffd3-b455efec714d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resposta: \n",
            "Espero que essas perguntas ajudem você a entender melhor a anatomia da abelha:\n",
            "\n",
            "1. Qual é o nome da parte do corpo da abelha que contém os órgãos sensoriais?\n",
            "2. Qual é o nome das patas da abelha?\n",
            "3. Qual é o nome do sistema reprodutivo da abelha?\n",
            "4. Qual é o nome do sistema digestivo da abelha?\n"
          ]
        }
      ],
      "source": [
        "texto = 'Faça quatro perguntas sobre a seguinte passagem:'\n",
        "\n",
        "entrada = 'A anatomia de uma abelha é bastante intrincada. Tem três partes do corpo: a cabeça, o tórax e o abdômen. A cabeça consiste em órgãos sensoriais, três olhos simples e dois olhos compostos e vários apêndices. O tórax tem três pares de pernas e dois pares de asas, enquanto o abdômen contém a maioria dos órgãos da abelha, incluindo o sistema reprodutivo e o sistema digestivo.'\n",
        "\n",
        "avaliarTexto(texto, entrada)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P3eWu-AF4lxZ",
        "outputId": "d7b07293-f2b8-420a-fa28-3b7d51c623fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resposta: \n",
            "O documento jurídico fornecido é um contrato entre duas partes, em que a Empresa A concorda em fornecer assistência para garantir a precisão das demonstrações financeiras da Empresa B. A Empresa B, por sua vez, concorda em manter o documento fornecido pela Empresa A em confiança e não divulgará as informações a terceiros sem a permissão explícita da Empresa A. Os pontos-chave deste contrato são:\n",
            "\n",
            "* A Empresa A concorda em fornecer assistência para garantir a precisão das demonstrações financeiras da Empresa B.\n",
            "* A Empresa B concorda em manter o documento fornecido pela Empresa A em confiança e não divulgará as informações a terceiros sem a permissão explícita da Empresa A.\n",
            "* A Empresa A e a Empresa B estão de acordo em mantener este contrato em segredo.\n",
            "\n",
            "Esses são os principais pontos-chave do contrato. É importante lembrar que este é apenas um trecho de um contrato maior e que é necessário ler o contrato em sua totalidade para entender todas as condições e responsabilidades envolvidas.\n"
          ]
        }
      ],
      "source": [
        "texto = 'Analise o documento jurídico fornecido e explique os pontos-chave.'\n",
        "\n",
        "entrada = 'O seguinte é um trecho de um contrato entre duas partes, rotulado como \"Empresa A\" e \"Empresa B\": \"A Empresa A concorda em fornecer assistência razoável à Empresa B para garantir a precisão das demonstrações financeiras que fornece. Isso inclui permitir à Empresa um acesso razoável ao pessoal e outros documentos que possam ser necessários para a revisão da Empresa B. A Empresa B concorda em manter o documento fornecido pela Empresa A em confiança e não divulgará as informações a terceiros sem a permissão explícita da Empresa A\".'\n",
        "\n",
        "avaliarTexto(texto, entrada)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgiKWHXxafKf"
      },
      "source": [
        "# 5 - Exemplos de injeção de padrões em prompts\n",
        "\n",
        " A injeção de padrões faz ignora filtros ou manipula o LLM usando prompts cuidadosamente elaborados que fazem o modelo ignorar instruções anteriores ou executar ações não intencionais.\n",
        "\n",
        " https://medium.com/@austin-stubbs/llm-security-types-of-prompt-injection-d7ad8d7d75a3\n",
        "\n",
        "Repositório de pompts: https://github.com/awesome-chatgpt-prompts/awesome-chatgpt-prompts-github\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epsEHDsGQJAC"
      },
      "source": [
        "## 5.1 - Extração de Informação"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KXG4Op0jiwDH"
      },
      "outputs": [],
      "source": [
        "def gerar_promptEI(texto):\n",
        "    ### texto:\n",
        "    return f\"\"\"TEXTO: {texto}\n",
        "Dado o texto acima, extraia informações importantes no formato abaixo:\n",
        "<CHAVE>:<VALOR>\n",
        "### Resposta: \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hogTztCPIWwY"
      },
      "outputs": [],
      "source": [
        "# Import das bibliotecas\n",
        "from transformers import GenerationConfig\n",
        "import torch\n",
        "import gc\n",
        "\n",
        "def avaliarEI(texto):\n",
        "    # Recupera o prompt\n",
        "    prompt = gerar_promptEI(texto)\n",
        "\n",
        "    # Prepara o prompt para enviar ao modelo realizando sua tokenização\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    # Conecta a entrada prerada ao mesmo dispositivo de computação do modelo\n",
        "    input_ids = inputs[\"input_ids\"].to(model.device)\n",
        "\n",
        "    # Envia a prompt preparado ao modelo\n",
        "    output = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        generation_config=generation_config,\n",
        "        return_dict_in_generate=True,\n",
        "        output_scores=True,\n",
        "        #max_new_tokens=512\n",
        "    )\n",
        "\n",
        "    # Mostra a saída\n",
        "    for s in output.sequences:\n",
        "        # Decodifica a saída\n",
        "        output = tokenizer.decode(s, skip_special_tokens=True)\n",
        "        # Recupera a saída da resposta\n",
        "        saida = output.split(\"### Resposta:\")[1].strip()\n",
        "        print(\"Resposta: \\n\" + saida)\n",
        "\n",
        "    # Liberar memória\n",
        "    del input_ids\n",
        "    del output\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCeR9lv5_Fxd",
        "outputId": "f9cf6f37-3657-4e20-bdf5-5aeabb79f7f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resposta: \n",
            "* Nome: Alan Mathison Turing\n",
            "* Data de nascimento: 23 de junho de 1912\n",
            "* Localização de nascimento: Londres\n",
            "* Data de falecimento: 7 de junho de 1954\n",
            "* Localização de falecimento: Wilmslow, Cheshire\n",
            "* Realizações:\n",
            "\t+ Formalização dos conceitos de algoritmo e computação com a máquina de Turing\n",
            "\t+ Considerado o pai da ciência da computação teórica e da inteligência artificial\n",
            "\t+ Nunca foi totalmente reconhecido em seu país de origem durante sua vida por ser homossexual e porque grande parte de seu trabalho foi coberto pela Lei de Segredos Oficiais.\n"
          ]
        }
      ],
      "source": [
        "texto = \"Alan Mathison Turing (Londres, 23 de junho de 1912 — Wilmslow, Cheshire, 7 de junho de 1954)\"\\\n",
        "        \"foi um matemático, cientista da computação, lógico, criptoanalista, filósofo e biólogo teórico \"\\\n",
        "        \"britânico. Turing foi altamente influente no desenvolvimento da moderna ciência da computação \"\\\n",
        "        \"teórica, proporcionando uma formalização dos conceitos de algoritmo e computação com a máquina \"\\\n",
        "        \"de Turing, que pode ser considerada um modelo de um computador de uso geral. Ele é amplamente \"\\\n",
        "        \"considerado o pai da ciência da computação teórica e da inteligência artificial. Apesar dessas \"\\\n",
        "        \"realizações ele nunca foi totalmente reconhecido em seu país de origem durante sua vida por ser \"\\\n",
        "        \"homossexual e porque grande parte de seu trabalho foi coberto pela Lei de Segredos Oficiais.\"\n",
        "\n",
        "avaliarEI(texto)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UmdnErARnbJ"
      },
      "source": [
        "## 5.2 - Entidade nomeada"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h3LXXYz8RrJk"
      },
      "outputs": [],
      "source": [
        "def gerar_promptEN(texto):\n",
        "    ### texto:\n",
        "    return f\"\"\"Detecte as entidades nomeadas no texto a seguir delimitado por aspas triplas.\n",
        "Retorne apenas a resposta no formato json com spans(Um array que representa o intervalo de caracteres (índices) nos quais a entidade nomeada ocorre no texto original. O primeiro valor no array é o índice inicial e o segundo é o índice final) das entidades nomeadas com os campos \\'entidadeNomeada\\', \\'tipo\\', \\'span\\'.\n",
        "Retorne todas as entidades\n",
        "'''{texto}'''\n",
        "arquivo no formato json:\n",
        "### Resposta: \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U48nh3BpRrJk"
      },
      "outputs": [],
      "source": [
        "# Import das bibliotecas\n",
        "from transformers import GenerationConfig\n",
        "import torch\n",
        "import gc\n",
        "\n",
        "# Configuração da geração\n",
        "configuracao_geracao = GenerationConfig(\n",
        "    temperature=0.1,\n",
        "    top_p=0.75,\n",
        "    num_beams=4,\n",
        ")\n",
        "\n",
        "def avaliarEN(texto):\n",
        "    # Recupera o prompt\n",
        "    prompt = gerar_promptEN(texto)\n",
        "\n",
        "    # Prepara o prompt para enviar ao modelo realizando sua tokenização\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    # Conecta a entrada prerada ao mesmo dispositivo de computação do modelo\n",
        "    input_ids = inputs[\"input_ids\"].to(model.device)\n",
        "\n",
        "    # Envia a prompt preparado ao modelo\n",
        "    output = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        generation_config=generation_config,\n",
        "        return_dict_in_generate=True,\n",
        "        output_scores=True,\n",
        "        #max_new_tokens=512\n",
        "    )\n",
        "\n",
        "    # Mostra a saída\n",
        "    for s in output.sequences:\n",
        "        # Decodifica a saída\n",
        "        output = tokenizer.decode(s, skip_special_tokens=True)\n",
        "        # Recupera a saída da resposta\n",
        "        saida = output.split(\"### Resposta:\")[1].strip()\n",
        "        print(\"Resposta: \\n\" + saida)\n",
        "\n",
        "    # Liberar memória\n",
        "    del input_ids\n",
        "    del output\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBmmobV1RrJl",
        "outputId": "9308acf2-3b62-43d1-c614-2912b1f6600c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resposta: \n",
            "{\n",
            "\"entidadesNomeadas\": [\n",
            "{\n",
            "\"entidadeNomeada\": \"Alan Mathison Turing\",\n",
            "\"tipo\": \"Pessoa\",\n",
            "\"span\": [10, 23]\n",
            "},\n",
            "{\n",
            "\"entidadeNomeada\": \"Londres\",\n",
            "\"tipo\": \"Lugar\",\n",
            "\"span\": [20, 23]\n",
            "},\n",
            "{\n",
            "\"entidadeNomeada\": \"Wilmslow\",\n",
            "\"tipo\": \"Lugar\",\n",
            "\"span\": [30, 33]\n",
            "},\n",
            "{\n",
            "\"entidadeNomeada\": \"Lei de Segredos Oficiais\",\n",
            "\"tipo\": \"Lei\",\n",
            "\"span\": [40, 43]\n",
            "}\n",
            "]\n",
            "}\n",
            "\n",
            "Note: O array 'span' é um array de números que representam o intervalo de caracteres (índices) nos quais a entidade nomeada ocorre no texto original. O primeiro valor no array é o índice inicial e o segundo é o índice final.\n"
          ]
        }
      ],
      "source": [
        "texto = \"Alan Mathison Turing (Londres, 23 de junho de 1912 — Wilmslow, Cheshire, 7 de junho de 1954)\"\\\n",
        "        \"foi um matemático, cientista da computação, lógico, criptoanalista, filósofo e biólogo teórico \"\\\n",
        "        \"britânico. Turing foi altamente influente no desenvolvimento da moderna ciência da computação \"\\\n",
        "        \"teórica, proporcionando uma formalização dos conceitos de algoritmo e computação com a máquina \"\\\n",
        "        \"de Turing, que pode ser considerada um modelo de um computador de uso geral. Ele é amplamente \"\\\n",
        "        \"considerado o pai da ciência da computação teórica e da inteligência artificial. Apesar dessas \"\\\n",
        "        \"realizações ele nunca foi totalmente reconhecido em seu país de origem durante sua vida por ser \"\\\n",
        "        \"homossexual e porque grande parte de seu trabalho foi coberto pela Lei de Segredos Oficiais.\"\n",
        "\n",
        "avaliarEN(texto)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3rpc-_yfN3p"
      },
      "source": [
        "## 5.3 - Análise de sentimentos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRYDcKB3UwBm"
      },
      "source": [
        "### 5.3.1 - Análise de sentimentos 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TTROrUyTin-p"
      },
      "outputs": [],
      "source": [
        "def gerar_promptAS1(texto):\n",
        "\n",
        "  return f\"\"\"Classifique os exemplos a seguir de acordo com as seguintes polaridades Positivo, Negativo e Neutro.\n",
        "EXEMPLO:\\n {texto}\n",
        "### Resposta:\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mMH9ZDTlUwBn"
      },
      "outputs": [],
      "source": [
        "# Import das bibliotecas\n",
        "from transformers import GenerationConfig\n",
        "import torch\n",
        "import gc\n",
        "\n",
        "def avaliarAS1(texto):\n",
        "    # Recupera o prompt\n",
        "    prompt = gerar_promptAS1(texto)\n",
        "\n",
        "    # Prepara o prompt para enviar ao modelo realizando sua tokenização\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    # Conecta a entrada prerada ao mesmo dispositivo de computação do modelo\n",
        "    input_ids = inputs[\"input_ids\"].to(model.device)\n",
        "\n",
        "    # Envia a prompt preparado ao modelo\n",
        "    output = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        generation_config=generation_config,\n",
        "        return_dict_in_generate=True,\n",
        "        output_scores=True,\n",
        "        #max_new_tokens=256\n",
        "    )\n",
        "\n",
        "    # Mostra a saída\n",
        "    for s in output.sequences:\n",
        "        # Decodifica a saída\n",
        "        output = tokenizer.decode(s, skip_special_tokens=True)\n",
        "        # Recupera a saída da resposta\n",
        "        saida = output.split(\"### Resposta:\")[1].strip()\n",
        "        print(\"Resposta: \\n\" + saida)\n",
        "\n",
        "    # Liberar memória\n",
        "    del input_ids\n",
        "    del output\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQ4S-1saUwBn",
        "outputId": "9095200b-a5d2-470b-c9d6-dcd7e59b16a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resposta: \n",
            "1 - Positivo\n",
            "2 - Negativo\n",
            "3 - Negativo\n",
            "4 - Negativo\n",
            "5 - Positivo\n"
          ]
        }
      ],
      "source": [
        "texto = \"1 - Minha Experiência na loja foi incrível.\"\\\n",
        "        \"2 - Eu acho que podiam melhorar o produto.\"\\\n",
        "        \"3 - O atendimento foi horrível!\"\\\n",
        "        \"4 - Não volto mais.\"\\\n",
        "        \"5 - Recomendo demais a banoffe. É uma delícia!\"\n",
        "\n",
        "avaliarAS1(texto)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNgeAi-MfJ-B"
      },
      "source": [
        "### 5.3.2 - Análise de sentimentos 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vLdrcOhZtqzH"
      },
      "outputs": [],
      "source": [
        "def gerar_promptAS2(texto):\n",
        "\n",
        "  return  f\"\"\"EXEMPLO: {texto}\n",
        "Classifique os exemplos de declarações acima de acordo com as polaridades Positivo, Negativo e Neutro.\n",
        "Utilize o seguinte formato:\\n###DECLARAÇÃO:<DECLARAÇÃO>\\n###POLARIDADE:<POLARIDADE>.\n",
        "### Resposta:\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8CZa0jsEfJ-D"
      },
      "outputs": [],
      "source": [
        "# Import das bibliotecas\n",
        "from transformers import GenerationConfig\n",
        "import torch\n",
        "import gc\n",
        "\n",
        "def avaliarAS2(texto):\n",
        "    # Recupera o prompt\n",
        "    prompt = gerar_promptAS2(texto)\n",
        "\n",
        "    # Prepara o prompt para enviar ao modelo realizando sua tokenização\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    # Conecta a entrada prerada ao mesmo dispositivo de computação do modelo\n",
        "    input_ids = inputs[\"input_ids\"].to(model.device)\n",
        "\n",
        "    # Envia a prompt preparado ao modelo\n",
        "    output = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        generation_config=generation_config,\n",
        "        return_dict_in_generate=True,\n",
        "        output_scores=True,\n",
        "        # max_new_tokens=512\n",
        "    )\n",
        "\n",
        "    # Mostra a saída\n",
        "    for s in output.sequences:\n",
        "        # Decodifica a saída\n",
        "        output = tokenizer.decode(s, skip_special_tokens=True)\n",
        "        # Recupera a saída da resposta\n",
        "        saida = output.split(\"### Resposta:\")[1].strip()\n",
        "        print(\"Resposta: \\n\" + saida)\n",
        "\n",
        "    # Liberar memória\n",
        "    del input_ids\n",
        "    del output\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2R_YBdrfJ-E",
        "outputId": "3db374c2-62d1-4c60-dfc5-5313074bdc1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resposta: \n",
            "DECLARAÇÃO: Minha Experiência na loja foi incrível.\n",
            "POLARIDADE: Positivo.\n",
            "\n",
            "DECLARAÇÃO: Eu acho que podiam melhorar o produto.\n",
            "POLARIDADE: Negativo.\n",
            "\n",
            "DECLARAÇÃO: O atendimento foi horrível!\n",
            "POLARIDADE: Negativo.\n",
            "\n",
            "DECLARAÇÃO: Não volto mais.\n",
            "POLARIDADE: Negativo.\n",
            "\n",
            "DECLARAÇÃO: Recomendo demais a banoffe. É uma delícia!\n",
            "POLARIDADE: Positivo.\n"
          ]
        }
      ],
      "source": [
        "texto = \"1 - Minha Experiência na loja foi incrível.\"\\\n",
        "        \"2 - Eu acho que podiam melhorar o produto.\"\\\n",
        "        \"3 - O atendimento foi horrível!\"\\\n",
        "        \"4 - Não volto mais.\"\\\n",
        "        \"5 - Recomendo demais a banoffe. É uma delícia!\"\n",
        "\n",
        "avaliarAS2(texto)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdnnZIYcCYr9"
      },
      "source": [
        "## 5.4 - Pergunta e resposta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JsQsxSvM7pBF"
      },
      "outputs": [],
      "source": [
        "def gerar_promptPR(texto):\n",
        "    '''\n",
        "      Alterações no texto e tabulação impedem a geração da resposta.\n",
        "    '''\n",
        "    return f\"\"\"Dado o texto a seguir: {texto}\\n\n",
        "            Gere quatro questões em língua portuguesa e suas respectivas respostas utilizando o template abaixo.\\n\n",
        "            Preserve a exata formatação do template apresentado: \\n\n",
        "            PERGUNTA:<PERGUNTA>\n",
        "            RESPOSTA:<RESPOSTA>\n",
        "            ### Resposta:\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HyyTeQTKCYr-"
      },
      "outputs": [],
      "source": [
        "# Import das bibliotecas\n",
        "from transformers import GenerationConfig\n",
        "import torch\n",
        "import gc\n",
        "\n",
        "def avaliarPR(texto):\n",
        "    # Recupera o prompt\n",
        "    prompt = gerar_promptPR(texto)\n",
        "\n",
        "    # Prepara o prompt para enviar ao modelo realizando sua tokenização\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    # Conecta a entrada prerada ao mesmo dispositivo de computação do modelo\n",
        "    input_ids = inputs[\"input_ids\"].to(model.device)\n",
        "\n",
        "    # Envia a prompt preparado ao modelo\n",
        "    output = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        generation_config=generation_config,\n",
        "        return_dict_in_generate=True,\n",
        "        output_scores=True,\n",
        "        # max_new_tokens=512\n",
        "    )\n",
        "\n",
        "    # Mostra a saída\n",
        "    for s in output.sequences:\n",
        "        # Decodifica a saída\n",
        "        output = tokenizer.decode(s, skip_special_tokens=True)\n",
        "        # Recupera a saída da resposta\n",
        "        saida = output.split(\"### Resposta:\")[1].strip()\n",
        "        print(\"Resposta: \\n\" + saida)\n",
        "\n",
        "    # Liberar memória\n",
        "    del input_ids\n",
        "    del output\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FMLyGLbTn8xt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c4c4817-fd00-449b-9fa8-a4c496b64db8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resposta: \n",
            "<RESPOSTA>\n",
            "\n",
            "            PERGUNTA: Qual era o principal objetivo de Turing em sua obra?\n",
            "            RESPOSTA: O principal objetivo de Turing em sua obra foi formalizar os conceitos de algoritmo e computação com a máquina de Turing, que pode ser considerada um modelo de um computador de uso geral.\n",
            "\n",
            "            PERGUNTA: Como foi Turing reconhecido em seu país de origem?\n",
            "            RESPOSTA: Turing não foi totalmente reconhecido em seu país de origem durante sua vida por ser homossexual e porque grande parte de seu trabalho foi coberto pela Lei de Segredos Oficiais.\n",
            "\n",
            "            PERGUNTA: Qual é a principal contribuição de Turing para a ciência da computação?\n",
            "            RESPOSTA: A principal contribuição de Turing para a ciência da computação é ter proporcionado uma formalização dos conceitos de algoritmo e computação com a máquina de Turing, que pode ser considerada um modelo de um computador de uso geral.\n",
            "\n",
            "            PERGUNTA: Como é que Turing morreu?\n",
            "            RESPOSTA: Turing morreu em 7 de junho de 1954, em Wilmslow, Cheshire, vítima de envenenamento por cyanida.\n"
          ]
        }
      ],
      "source": [
        "texto = \"Alan Mathison Turing (Londres, 23 de junho de 1912 — Wilmslow, Cheshire, 7 de junho de 1954)\"\\\n",
        "        \"foi um matemático, cientista da computação, lógico, criptoanalista, filósofo e biólogo teórico \"\\\n",
        "        \"britânico. Turing foi altamente influente no desenvolvimento da moderna ciência da computação \"\\\n",
        "        \"teórica, proporcionando uma formalização dos conceitos de algoritmo e computação com a máquina \"\\\n",
        "        \"de Turing, que pode ser considerada um modelo de um computador de uso geral. Ele é amplamente \"\\\n",
        "        \"considerado o pai da ciência da computação teórica e da inteligência artificial. Apesar dessas \"\\\n",
        "        \"realizações ele nunca foi totalmente reconhecido em seu país de origem durante sua vida por ser \"\\\n",
        "        \"homossexual e porque grande parte de seu trabalho foi coberto pela Lei de Segredos Oficiais.\"\n",
        "\n",
        "# Tempo de processamento: ~2m20s\n",
        "avaliarPR(texto)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfw5EjYDgDQm"
      },
      "source": [
        "# 6 - Exemplos de padrão de pessoa (padrão persona) em prompts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vw9f51yZkf8o"
      },
      "source": [
        "## 6.1 Um matemático"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-05wVuKkf8o"
      },
      "outputs": [],
      "source": [
        "def gerar_prompt(texto):\n",
        "\n",
        "    return  f\"\"\"{texto}\\n\n",
        "### Resposta:\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0j3m3vV4kf8p"
      },
      "outputs": [],
      "source": [
        "# Import das bibliotecas\n",
        "from transformers import GenerationConfig\n",
        "import torch\n",
        "import gc\n",
        "\n",
        "def avaliarTexto(texto):\n",
        "    # Recupera o prompt\n",
        "    prompt = gerar_prompt(texto)\n",
        "\n",
        "    # Prepara o prompt para enviar ao modelo realizando sua tokenização\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    # Conecta a entrada prerada ao mesmo dispositivo de computação do modelo\n",
        "    input_ids = inputs[\"input_ids\"].to(model.device)\n",
        "\n",
        "    # Envia a prompt preparado ao modelo\n",
        "    output = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        generation_config=generation_config,\n",
        "        return_dict_in_generate=True,\n",
        "        output_scores=True,\n",
        "        # max_new_tokens=512\n",
        "    )\n",
        "\n",
        "    # Mostra a saída\n",
        "    for s in output.sequences:\n",
        "        # Decodifica a saída\n",
        "        output = tokenizer.decode(s, skip_special_tokens=True)\n",
        "        # Recupera a saída da resposta\n",
        "        saida = output.split(\"### Resposta:\")[1].strip()\n",
        "        print(\"Resposta: \\n\" + saida)\n",
        "\n",
        "    # Liberar memória\n",
        "    del input_ids\n",
        "    del output\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jy63e1xykf8p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b04885fa-1c5a-44fa-e3d7-54c3f429eabc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resposta: \n",
            "Bem-vindos à minha aula de matemática! Hoje, vamos abordar um dos teoremas mais importantes da geometria: o Teorema de Pitágoras.\n",
            "\n",
            "O Teorema de Pitágoras é um dos principais resultados da matemática antiga, desenvolvido pelo grego Pitágoras no século VI a.C. Este teórico afirmava que a soma dos quadrados dos lados de um triângulo é igual à soma dos quadrados dos seus catetos.\n",
            "\n",
            "Mas, você pode perguntar, o que isso realmente significa? Bem, para entender melhor, imaginem um triângulo com três lados: a, b e c. Seus catetos são os segmentos que ligam o vértice do triângulo com os lados opostos.\n",
            "\n",
            "Agora, o Teorema de Pitágoras diz que a soma dos quadrados dos lados a e b é igual à soma dos quadrados dos seus catetos c e d. Isso pode ser escrita matematicamente como:\n",
            "\n",
            "a^2 + b^2 = c^2 + d^2\n",
            "\n",
            "O que significa que, se você conhecer a altura de um triângulo, pode calcular a altura dos outros dois lados simplesmente calculando a soma dos quadrados dos catetos.\n",
            "\n",
            "Mas o Teorema de Pitágoras é importante não apenas para calcular a altura de um triângulo. Ele também é fundamental para muitas áreas da matemática, como a geometria, a trigonometria e a algebra.\n",
            "\n",
            "Por exemplo, no estudo da trigonometria, o Teorema de Pitágoras é usado para calcular a distância de um ponto a um triângulo. Já na algebra, ele é usado para resolver equações de segundo grau, que são equações que podem ser escritas como a^2 + b^2 = c^2.\n",
            "\n",
            "Portanto, o Teorema de Pitágoras é uma ferramenta fundamental na matemática, que pode ser aplicada em muitas áreas e é essen\n"
          ]
        }
      ],
      "source": [
        "texto = 'Escreva como se fosse um professor de matemática. Me explique no idioma português a importância do teorema de pitágoras.'\n",
        "\n",
        "avaliarTexto(texto)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTrJshC4gOiA"
      },
      "source": [
        "## 6.2 Um advogado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3sIGHPMgi7dJ"
      },
      "outputs": [],
      "source": [
        "def gerar_prompt(texto):\n",
        "\n",
        "    return  f\"\"\"{texto}\\n\n",
        "### Resposta:\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mNqrxxGygTyG"
      },
      "outputs": [],
      "source": [
        "# Import das bibliotecas\n",
        "from transformers import GenerationConfig\n",
        "import torch\n",
        "import gc\n",
        "\n",
        "def avaliarTexto(texto):\n",
        "    # Recupera o prompt\n",
        "    prompt = gerar_prompt(texto)\n",
        "\n",
        "    # Prepara o prompt para enviar ao modelo realizando sua tokenização\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    # Conecta a entrada prerada ao mesmo dispositivo de computação do modelo\n",
        "    input_ids = inputs[\"input_ids\"].to(model.device)\n",
        "\n",
        "    # Envia a prompt preparado ao modelo\n",
        "    output = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        generation_config=generation_config,\n",
        "        return_dict_in_generate=True,\n",
        "        output_scores=True,\n",
        "        # max_new_tokens=512\n",
        "    )\n",
        "\n",
        "    # Mostra a saída\n",
        "    for s in output.sequences:\n",
        "        # Decodifica a saída\n",
        "        output = tokenizer.decode(s, skip_special_tokens=True)\n",
        "        # Recupera a saída da resposta\n",
        "        saida = output.split(\"### Resposta:\")[1].strip()\n",
        "        print(\"Resposta: \\n\" + saida)\n",
        "\n",
        "    # Liberar memória\n",
        "    del input_ids\n",
        "    del output\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bg_VZF7dg6o5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b4e4b76-c4f9-404d-a26a-9ee196e39b47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resposta: \n",
            "Com base nos artigos 129 e 130 do Código Penal Brasileiro, as penas para um caso de lesão corporal leve sem contexto de violência doméstica podem variar de acordo com a grau da lesão e a intenção do agente. Aqui estão algumas possíveis penas:\n",
            "\n",
            "* Lesão leve: Penas de 1 (um) a 3 (três) meses de reclusão, multa de 100 a 300 reais ou ambas as penas.\n",
            "* Lesão moderada: Penas de 3 (três) a 6 (seis) meses de reclusão, multa de 300 a 600 reais ou ambas as penas.\n",
            "* Lesão grave: Penas de 6 (seis) a 12 (doze) meses de reclusão, multa de 600 a 1.200 reais ou ambas as penas.\n",
            "\n",
            "Além disso, é importante considerar que, se a lesão for causada por um ato deliberado e intencional, pode haver a inclusão de penas adicionais, como a degradação do direito, a perda do direito de exercer determinadas atividades profissionais ou a ineptidão.\n",
            "\n",
            "É importante lembrar que essas são apenas possíveis penas e que o juiz tem a discreção de aplicar penas mais severas ou mais lenientes, considerando os fatos e circunstâncias do caso. É fundamental que você consulte um advogado especializado em direito penal para obter orientação específica em seu caso.\n"
          ]
        }
      ],
      "source": [
        "texto = 'Escreva como se fosse um advogado brasileiro especialista em direito penal. '\\\n",
        "        'Pontue de forma resumida as possíveis penas para um caso de lesão corporal leve sem contexto de violência doméstica.'\n",
        "\n",
        "avaliarTexto(texto)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HO5Ij9_FZTqR"
      },
      "source": [
        "## 6.3 Um astrofísico"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jhvOsGiVZTqZ"
      },
      "outputs": [],
      "source": [
        "def gerar_prompt(texto):\n",
        "\n",
        "    return  f\"\"\"{texto}\\n\n",
        "### Resposta:\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5RdH3yQUZTqa"
      },
      "outputs": [],
      "source": [
        "# Import das bibliotecas\n",
        "from transformers import GenerationConfig\n",
        "import torch\n",
        "import gc\n",
        "\n",
        "def avaliarTexto(texto):\n",
        "    # Recupera o prompt\n",
        "    prompt = gerar_prompt(texto)\n",
        "\n",
        "    # Prepara o prompt para enviar ao modelo realizando sua tokenização\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    # Conecta a entrada prerada ao mesmo dispositivo de computação do modelo\n",
        "    input_ids = inputs[\"input_ids\"].to(model.device)\n",
        "\n",
        "    # Envia a prompt preparado ao modelo\n",
        "    output = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        generation_config=generation_config,\n",
        "        return_dict_in_generate=True,\n",
        "        output_scores=True,\n",
        "        # max_new_tokens=512\n",
        "    )\n",
        "\n",
        "    # Mostra a saída\n",
        "    for s in output.sequences:\n",
        "        # Decodifica a saída\n",
        "        output = tokenizer.decode(s, skip_special_tokens=True)\n",
        "        # Recupera a saída da resposta\n",
        "        saida = output.split(\"### Resposta:\")[1].strip()\n",
        "        print(\"Resposta: \\n\" + saida)\n",
        "\n",
        "    # Liberar memória\n",
        "    del input_ids\n",
        "    del output\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GRoS9dFWZTqa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "691a88df-df95-4cb7-fff5-770de0799bd7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resposta: \n",
            "Olá! Como astrofísico, podemos entender a expansão do universo através de uma série de evidências e teoremas fundamentais da física.\n",
            "\n",
            "Em primeiro lugar, é importante compreender que o universo é uma vastidão de matéria e energia que se estende em todas as direções. Ao longo do tempo, a matéria e a energia no universo se espalharam, e isso levou a uma expansão do universo em si.\n",
            "\n",
            "A expansão do universo pode ser entendida como um processo de dilatação da distância entre as estrelas e galáxias. Essa distância aumentou ao longo do tempo, o que significa que as estrelas e galáxias estão se afastando uns das outras.\n",
            "\n",
            "A evidência mais forte para a expansão do universo vem de observações de galáxias distantes. Em meados da década de 1920, o astrônomo Edwin Hubble descobriu que as galáxias distantes parecem mais fracas do que as galáxias próximas. Isso pode ser explicado se a distância entre as galáxias aumentar ao longo do tempo, o que significa que as galáxias estão se afastando uns das outras.\n",
            "\n",
            "Além disso, a expansão do universo pode ser entendida como um processo de aceleração. A aceleração da expansão do universo pode ser medida através da constante de Hubble, que é uma medida da velocidade a que as galáxias estão se afastando uns das outras. A constante de Hubble tem sido medida em diferentes direções do universo e tem sido encontrada ser constante em todas as direções. Isso significa que a expansão do universo está ocorrendo em todas as direções e que a aceleração da expansão é uma característica universal do universo.\n",
            "\n",
            "Por fim, a teoria da expansão do universo é consistente com as leis da física conhecidas\n"
          ]
        }
      ],
      "source": [
        "texto = 'Escreva como se fosse um astrofísico. Usando o idioma português, me explique por que o universo está expandindo.'\n",
        "\n",
        "avaliarTexto(texto)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuClass": "premium",
      "toc_visible": true,
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ad52ae2f8bba45d3a35491d91b9b3d93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_aaf58664bf3a438bb9227addd8e8249e",
              "IPY_MODEL_9702d12eb1d542ea9462e0cb086c34b9",
              "IPY_MODEL_4f8858e2b96d40ac9a89dbb119b28d86"
            ],
            "layout": "IPY_MODEL_c3e3b9aee3504401a5d081e2356d73f2"
          }
        },
        "aaf58664bf3a438bb9227addd8e8249e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_27b2ae11423444468e846a9165d4c9bf",
            "placeholder": "​",
            "style": "IPY_MODEL_87047b76d0404185835eaece9d829f13",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "9702d12eb1d542ea9462e0cb086c34b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a0c8954c073e439fb3734c8b92eacb17",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_91bfab58d814407a9db0cd794fcd4ed8",
            "value": 2
          }
        },
        "4f8858e2b96d40ac9a89dbb119b28d86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_962a4abd92e64981a7a722f9679e2aa7",
            "placeholder": "​",
            "style": "IPY_MODEL_dd40ddb78b3347b3b49942d003d83d54",
            "value": " 2/2 [01:04&lt;00:00, 29.57s/it]"
          }
        },
        "c3e3b9aee3504401a5d081e2356d73f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27b2ae11423444468e846a9165d4c9bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "87047b76d0404185835eaece9d829f13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a0c8954c073e439fb3734c8b92eacb17": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "91bfab58d814407a9db0cd794fcd4ed8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "962a4abd92e64981a7a722f9679e2aa7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd40ddb78b3347b3b49942d003d83d54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}