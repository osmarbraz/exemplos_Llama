{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/osmarbraz/exemplos_Llama/blob/main/ExemplosGeracaoTexto_Llama2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78HE8FLsKN9Q"
      },
      "source": [
        "#Exemplo de Geração de textos usando Llama v2.0 usando Transformers by HuggingFace\n",
        "\n",
        "Exemplo de uso do modelo de linguagem grande Llama v2.0.\n",
        "- Analise da geração de textos\n",
        "- Prompts com textos emparelhados\n",
        "- Injentando padrões no prompt\n",
        "- Padrão Persona\n",
        "\n",
        "**Toda a execução ocorre no Google Colaboratory.**\n",
        "\n",
        "Pré-requisitos:\n",
        "- Lhama 2 não está acessível abertamente e requer solicitação  de acesso. Faça o cadastro no site do https://huggingface.co/join. Depois do login, gere um token de acesso no link https://huggingface.co/settings/tokens.\n",
        "- Configurar o notebook para usar GPU- Acesse o menu 'Ambiente de Execução -> Alterar o tipo do ambiente de execução -> Acelerador de hardware -> T4 GPU\n",
        "\n",
        "\n",
        "**Notebook de referência:**\n",
        "\n",
        "https://github.com/guardiaum/tutorial-sbbd2023/blob/main/Prompt_Engineering.ipynb\n",
        "\n",
        "\n",
        "**Lista dos modelos:**\n",
        "\n",
        "https://huggingface.co/models\n",
        "\n",
        "\n",
        "**Artigos referências:**\n",
        "\n",
        "https://dev.to/nithinibhandari1999/how-to-run-llama-2-on-your-local-computer-42g1\n",
        "\n",
        "\n",
        "**Link biblioteca Huggingface:**\n",
        "\n",
        "https://github.com/huggingface/transformers\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyxb5Px3p1-e"
      },
      "source": [
        "# 0 - Preparação do ambiente\n",
        "Preparação do ambiente para execução do exemplo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAPVtRXQqDim"
      },
      "source": [
        "## Tratamento de logs\n",
        "\n",
        "Método para tratamento dos logs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DcopxbGZqDip"
      },
      "outputs": [],
      "source": [
        "# Biblioteca de logging\n",
        "import logging\n",
        "\n",
        "# Formatando a mensagem de logging\n",
        "logging.basicConfig(format=\"%(asctime)s : %(levelname)s : %(message)s\", level=logging.INFO)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GjYtXcMnSAe"
      },
      "source": [
        "## Identificando o ambiente Colab\n",
        "\n",
        "Cria uma variável para identificar que o notebook está sendo executado no Google Colaboratory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YMiH0E3OnRa1"
      },
      "outputs": [],
      "source": [
        "# Se estiver executando no Google Colaboratory\n",
        "import sys\n",
        "\n",
        "# Retorna true ou false se estiver no Google Colaboratory\n",
        "IN_COLAB = \"google.colab\" in sys.modules"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AoYjkIZfXgP"
      },
      "source": [
        "## Funções auxiliares"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "603LYIYKBmq5"
      },
      "source": [
        "Função auxiliar para formatar o tempo como `hh: mm: ss`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Guy6B4whsZFR"
      },
      "outputs": [],
      "source": [
        "# Import das bibliotecas.\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "def formataTempo(tempo):\n",
        "    \"\"\"\n",
        "      Pega a tempo em segundos e retorna uma string hh:mm:ss\n",
        "    \"\"\"\n",
        "    # Arredonda para o segundo mais próximo.\n",
        "    tempo_arredondado = int(round((tempo)))\n",
        "\n",
        "    # Formata como hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=tempo_arredondado))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pqa-7WXBAw8q"
      },
      "source": [
        "# 1 - Instalação das bibliotecas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zp0jVfo3QM3h"
      },
      "source": [
        "O bitsandbytes é um wrapper leve em torno de funções personalizadas CUDA, em particular otimizadores de 8 bits, multiplicação de matrizes (LLM.int8()) e funções de quantização. É uma dependência do accelerate.\n",
        "\n",
        "https://huggingface.co/blog/4bit-transformers-bitsandbytes\n",
        "\n",
        "https://pypi.org/project/bitsandbytes/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12GE2W3fQM_n",
        "outputId": "27dc7e7d-1d4d-4866-ce7c-32436b54b52e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bitsandbytes==0.41.1 in /usr/local/lib/python3.10/dist-packages (0.41.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install bitsandbytes==0.41.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7wU6vuyAuPd"
      },
      "source": [
        "Accelerate é uma biblioteca que permite que o mesmo código PyTorch seja executado em qualquer configuração distribuída adicionando apenas quatro linhas de código. Otimiza as operações do PyTorch, especialmente na GPU.\n",
        "\n",
        "https://pypi.org/project/accelerate/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qTMID1rZAvx7",
        "outputId": "f05f2208-1760-4334-fb0c-b2e53ab1c5d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: accelerate==0.23.0 in /usr/local/lib/python3.10/dist-packages (0.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.23.0) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.23.0) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.23.0) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate==0.23.0) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.23.0) (2.1.0+cu118)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate==0.23.0) (0.18.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.23.0) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.23.0) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.23.0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.23.0) (3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.23.0) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.23.0) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.23.0) (2.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate==0.23.0) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate==0.23.0) (4.66.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.23.0) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.23.0) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.23.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.23.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.23.0) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate==0.23.0) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install accelerate==0.23.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCdqJCtQN52l"
      },
      "source": [
        "A Biblioteca A Biblioteca Transformers fornece APIs e ferramentas para baixar e treinar facilmente modelos pré-treinados de última geração para Processamento de linguagem natural, Visão computacional, Áudio, etc.\n",
        "\n",
        "Fornece uma maneira direta de usar modelos pré-treinados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1RfUN_KolV-f",
        "outputId": "48fc09c6-963a-4197-8bc8-45235acb55c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers==4.31.0 in /usr/local/lib/python3.10/dist-packages (4.31.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (3.12.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (0.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.31.0) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.31.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.31.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.31.0) (2023.7.22)\n"
          ]
        }
      ],
      "source": [
        "# Instala a última versão da biblioteca\n",
        "# !pip install transformers\n",
        "\n",
        "# A última versão do huggingface apresenta um problema:\n",
        "# UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1`\n",
        "# https://discuss.huggingface.co/t/help-with-llama-2-finetuning-setup/50035\n",
        "# Usar a versão 4.31.0\n",
        "\n",
        "# Instala uma versão específica da biblioteca\n",
        "!pip install -U transformers==4.31.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlrWrRP02tuZ"
      },
      "source": [
        "A Biblioteca huggingface-cli fornece vários comandos para interagir com o Hugging Face Hub a partir da linha de comando. Um desses comandos é o login, que permite aos usuários se autenticarem no Hub usando suas credenciais."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQxtD3Zk14ov",
        "outputId": "67f486b9-253f-4eed-dd2e-7978db5aabfc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub==0.18.0 in /usr/local/lib/python3.10/dist-packages (0.18.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub==0.18.0) (3.12.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub==0.18.0) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub==0.18.0) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub==0.18.0) (4.66.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub==0.18.0) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub==0.18.0) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub==0.18.0) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub==0.18.0) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub==0.18.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub==0.18.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub==0.18.0) (2023.7.22)\n"
          ]
        }
      ],
      "source": [
        "!pip install huggingface_hub==0.18.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bcpd9t9PpkrX"
      },
      "source": [
        "# 2 - Carregando o LLM\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFRSYoCArrQ-"
      },
      "source": [
        "## 2.1 - Login no huggingface\n",
        "\n",
        "- Lhama 2 não está acessível abertamente e requer solicitação  de acesso. Faça o cadastro no site do https://huggingface.co/join. Depois do login, gere um token de acesso no link https://huggingface.co/settings/tokens.\n",
        "\n",
        "Insira o token quando solicitado e depois digite Y para adicionar as credenciais."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0bkqIoNU18UH"
      },
      "outputs": [],
      "source": [
        "# !huggingface-cli login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACJuj9wB9kjZ"
      },
      "source": [
        "Se o seu notebook não for público e não desejar incluir o token de acesso toda vez que for executar o notebook preencha o método save_token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lRVr7uqp9Ubk"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub.hf_api import HfFolder\n",
        "\n",
        "ACCESS_TOKEN  = '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'\n",
        "\n",
        "HfFolder.save_token(ACCESS_TOKEN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIzrrJLw9oQd"
      },
      "source": [
        "Mostrando o usuário conectado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xLrSstlxR_kq",
        "outputId": "25ef22f5-bbaf-4224-ad30-925ee0d893f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "osmarbraz\n"
          ]
        }
      ],
      "source": [
        "!huggingface-cli whoami"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niwUEmYM6kjG"
      },
      "source": [
        "## 2.2 - Nome do modelo de linguagem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBOzL86X6kjM"
      },
      "source": [
        "Define o nome do modelo a ser carregado\n",
        "Lista dos modelos:\n",
        "  - https://huggingface.co/meta-llama/Llama-2-7b-hf\n",
        "  - https://huggingface.co/meta-llama/Llama-2-7b-chat-hf\n",
        "  - https://huggingface.co/meta-llama/Llama-2-13b-hf\n",
        "  - https://huggingface.co/meta-llama/Llama-2-13b-chat-hf\n",
        "  - https://huggingface.co/meta-llama/Llama-2-70b-hf\n",
        "  - https://huggingface.co/meta-llama/Llama-2-70b-chat-hf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-zOnSymM6kjM"
      },
      "outputs": [],
      "source": [
        "#nome_modelo = \"meta-llama/Llama-2-7b-hf\"\n",
        "nome_modelo = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "\n",
        "# Não carrega por falta de memória no google colab\n",
        "#nome_modelo = \"meta-llama/Llama-2-13b-hf\"\n",
        "#nome_modelo = \"meta-llama/Llama-2-13b-chat-hf\"\n",
        "\n",
        "# Não carrega por falta de memória e espaço em disco no google colab\n",
        "#nome_modelo = \"meta-llama/Llama-2-70b-hf\"\n",
        "#nome_modelo = \"meta-llama/Llama-2-70b-chat-hf\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzWcQNSORrYC"
      },
      "source": [
        "## 2.3 - Carrega o tokenizador\n",
        "\n",
        "Carregando o **tokenizador** da comunidade."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MlSM1VufRw5B",
        "outputId": "a9864282-8f4a-4c65-ec0d-63d77b90032f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Carregando o tokenizador meta-llama/Llama-2-7b-chat-hf da comunidade...\n"
          ]
        }
      ],
      "source": [
        "# Importando as bibliotecas do Tokenizador\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Carregando o Tokenizador da comunidade\n",
        "print('Carregando o tokenizador ' + nome_modelo + ' da comunidade...')\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(nome_modelo)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tamanho do vocabulário"
      ],
      "metadata": {
        "id": "pNhZxBfM0LEe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(tokenizer))"
      ],
      "metadata": {
        "id": "IzgbIOUI0LEf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc9acb58-f936-4263-ed42-c3531d4f3512"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhNMEhN9BHuc"
      },
      "source": [
        "## 2.4 - Carregando o Modelo LLM\n",
        "\n",
        "Carregando o **modelo** da comunidade.\n",
        "\n",
        "Parametrização do from_pretrained\n",
        "https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "d26688da76da408c94ee367cd3d15a99",
            "b7db995eeb4c4ec3924cdc62fcd9730a",
            "bd9108bb036a418992a533cc497f5abe",
            "f64dffc5a64246a2832a405f80305bf5",
            "e4dc53314bb84e56ac0257f0574e516c",
            "79a80962f9fa4b5aaa3ce5f32e2f4b76",
            "0125381a28f34a3cb2dca74ddaba53eb",
            "a18d997034714c28878290cdb73f5ecc",
            "fe5ca8462cde4ce29e51dff290beb630",
            "0786b8a01db044008f5db726686651d4",
            "44e0f1ab21634065b13641bd3e3c6c49"
          ]
        },
        "id": "zH_tnwWJRnQL",
        "outputId": "32f8d58d-b8a3-450e-ae6f-ce09fe206005"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Carregando o modelo meta-llama/Llama-2-7b-chat-hf da comunidade...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d26688da76da408c94ee367cd3d15a99"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tempo de carregamento do modelo:  0:01:20 (h:mm:ss)\n"
          ]
        }
      ],
      "source": [
        "# Importando as bibliotecas do Modelo\n",
        "from transformers import AutoModelForCausalLM\n",
        "import time\n",
        "\n",
        "# Guarda o tempo de início do carregamento do modelo\n",
        "tempo_inicio = time.time()\n",
        "\n",
        "# Carregando o Modelo da comunidade\n",
        "print('Carregando o modelo ' + nome_modelo + ' da comunidade...')\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(nome_modelo,\n",
        "                                             #torch_dtype=torch.float16, #default\n",
        "                                             trust_remote_code=True,   # Carrega de um repositório confiável\n",
        "                                             load_in_8bit=True, # Reduz o consumo de memória em aproximadamente metade\n",
        "                                             device_map=\"auto\"\n",
        "                                             )\n",
        "\n",
        "# Coloca o modelo e modo avaliação\n",
        "model.eval()\n",
        "\n",
        "print(\"Tempo de carregamento do modelo:  {:} (h:mm:ss)\".format(formataTempo(time.time() - tempo_inicio)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rwKEcUH1ThzA",
        "outputId": "92b3f3d5-4781-4c24-fd7b-92d8e38111d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LlamaForCausalLM(\n",
            "  (model): LlamaModel(\n",
            "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
            "    (layers): ModuleList(\n",
            "      (0-31): 32 x LlamaDecoderLayer(\n",
            "        (self_attn): LlamaAttention(\n",
            "          (q_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
            "          (k_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
            "          (v_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
            "          (o_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
            "          (rotary_emb): LlamaRotaryEmbedding()\n",
            "        )\n",
            "        (mlp): LlamaMLP(\n",
            "          (gate_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
            "          (up_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
            "          (down_proj): Linear8bitLt(in_features=11008, out_features=4096, bias=False)\n",
            "          (act_fn): SiLUActivation()\n",
            "        )\n",
            "        (input_layernorm): LlamaRMSNorm()\n",
            "        (post_attention_layernorm): LlamaRMSNorm()\n",
            "      )\n",
            "    )\n",
            "    (norm): LlamaRMSNorm()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXgoG2ZvuHFI",
        "outputId": "e3c720de-0e55-41d4-8864-940fe6ec7209"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LlamaConfig {\n",
            "  \"_name_or_path\": \"meta-llama/Llama-2-7b-chat-hf\",\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 11008,\n",
            "  \"max_position_embeddings\": 4096,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 32,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"quantization_config\": {\n",
            "    \"bnb_4bit_compute_dtype\": \"float32\",\n",
            "    \"bnb_4bit_quant_type\": \"fp4\",\n",
            "    \"bnb_4bit_use_double_quant\": false,\n",
            "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
            "    \"llm_int8_has_fp16_weight\": false,\n",
            "    \"llm_int8_skip_modules\": null,\n",
            "    \"llm_int8_threshold\": 6.0,\n",
            "    \"load_in_4bit\": false,\n",
            "    \"load_in_8bit\": true\n",
            "  },\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float16\",\n",
            "  \"transformers_version\": \"4.31.0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(model.config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ysqp5fuyRWc4",
        "outputId": "6e133081-ef68-4164-febc-a21be528ae40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4096\n"
          ]
        }
      ],
      "source": [
        "print(model.config.max_position_embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tamanho do vocabulário"
      ],
      "metadata": {
        "id": "mpGMYgt6zWtX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.config.vocab_size)"
      ],
      "metadata": {
        "id": "ZT7nQq3Q0ALQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab4852bd-3477-408f-a1d0-72ec55352c06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.5 - Configuração da geração de texto"
      ],
      "metadata": {
        "id": "Fjs8uvajG5gc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import das bibliotecas\n",
        "from transformers import GenerationConfig\n",
        "\n",
        "# Instância as configurações do modelo\n",
        "generation_config = GenerationConfig.from_pretrained(nome_modelo)\n",
        "\n",
        "print(\"GenerationConfig antes:\\n\",generation_config)\n",
        "generation_config.max_new_tokens = 2048 #Preenche até um comprimento máximo especificado com o argumento max_length ou até o comprimento de entrada máximo aceitável para o modelo se esse argumento não for fornecido.\n",
        "#generation_config.max_length = 4096 # (Default 4096)\n",
        "generation_config.temperature = 0.1 # (Default 0.6) A temperatura é um parâmetro que controla a aleatoriedade da saída do LLM. Uma temperatura mais alta resultará em um texto mais criativo e imaginativo, enquanto uma temperatura mais baixa resultará em um texto mais preciso e factual.\n",
        "#generation_config.top_k = 3  # Top-k diz ao modelo para escolher o próximo token entre os 'k' tokens principais de sua lista, classificados por probabilidade.\n",
        "#generation_config.top_p = 0.9 # (Default 0.9) Top-p é mais dinâmico que top-k e é frequentemente usado para excluir resultados com probabilidades mais baixas. Portanto, se você definir p como 0,75, excluirá os 25% inferiores dos resultados prováveis.\n",
        "#generation_config.do_sample = True # (Default True) Se definido como True, este parâmetro permite estratégias de decodificação como amostragem multinomial, amostragem multinomial de busca de feixe, amostragem Top-K e amostragem Top-p. Todas essas estratégias selecionam o próximo token da distribuição de probabilidade em todo o vocabulário com vários ajustes específicos da estratégia.\n",
        "#generation_config.repetition_penalty = 1.20 # Penaliza a repetição e visa evitar frases que se repetem sem nada de realmente interessante.\n",
        "#generation_config.num_return_sequences=1, # Retorna uma única sentença da saída.\n",
        "print(\"GenerationConfig depois:\\n\",generation_config)"
      ],
      "metadata": {
        "id": "LmIPtO-RG-_b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85300c07-0ee8-4964-8d8d-39c614a8b8db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GenerationConfig antes:\n",
            " GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"do_sample\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"max_length\": 4096,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"temperature\": 0.6,\n",
            "  \"top_p\": 0.9,\n",
            "  \"transformers_version\": \"4.31.0\"\n",
            "}\n",
            "\n",
            "GenerationConfig depois:\n",
            " GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"do_sample\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"max_length\": 4096,\n",
            "  \"max_new_tokens\": 2048,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"temperature\": 0.1,\n",
            "  \"top_p\": 0.9,\n",
            "  \"transformers_version\": \"4.31.0\"\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qezcBkxnEdR"
      },
      "source": [
        "# 3 - Analisando a geração de textos\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4Pd6-h0YD8U"
      },
      "source": [
        "## 3.1 - Geração de texto\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define o documento"
      ],
      "metadata": {
        "id": "6xxMMYLV1Dzm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8QP-2tC8YOFW",
        "outputId": "8d9a9bc8-ede1-4191-9239-6d2d83972950"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 <s>\n",
            "1 ▁Como\n",
            "2 ▁emp\n",
            "3 il\n",
            "4 har\n",
            "5 ▁elementos\n",
            "6 ▁em\n",
            "7 ▁uma\n",
            "8 ▁pil\n",
            "9 ha\n",
            "10 ?\n"
          ]
        }
      ],
      "source": [
        "# Define o documento base\n",
        "documento = \"Como empilhar elementos em uma pilha?\"\n",
        "#documento = \"How to push elements in a stack\"\n",
        "#documento = \"O comando SQL para extrair todos os usuários cujo nome começa com A é:\"\n",
        "#documento = \"Bom dia professor, tudo bem ?\"\n",
        "# documento = \"The SQL command to extract all the users whose name starts with A is:\"\n",
        "#documento = \"How to push elements in a stack\"\n",
        "#documento = \"Write code for finding the prime number in python ?\"\n",
        "# documento = \"Escrever código para encontrar o número primo em python?\"\n",
        "\n",
        "# Prepara o prompt para enviar ao modelo realizando sua tokenização\n",
        "# Se pt for especificado, ele retornará tensores em vez de lista de inteiros python e tokenizará os documentos\n",
        "input = tokenizer(documento, return_tensors=\"pt\")\n",
        "\n",
        "# Mostra os tokens com seus índices\n",
        "i = 0\n",
        "for tup in input.input_ids[0]:\n",
        "    # print(tup.item())\n",
        "    print(\"{} {}\".format(i, tokenizer.convert_ids_to_tokens(tup.item())))\n",
        "    i= i + 1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configura e envia o texto ao LLM"
      ],
      "metadata": {
        "id": "LTpS6fNC1F_m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6sMpJpeewC-i"
      },
      "outputs": [],
      "source": [
        "# Import das bibliotecas\n",
        "from transformers import GenerationConfig\n",
        "import torch\n",
        "import gc\n",
        "\n",
        "# Conecta a entrada prerada ao mesmo dispositivo de computação do modelo\n",
        "input_ids = input[\"input_ids\"].to(model.device)\n",
        "\n",
        "# Envia a prompt preparado ao modelo\n",
        "outputs = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        generation_config=generation_config, # Passa as configurações da geração de texto para o modelo\n",
        "        # https://huggingface.co/docs/transformers/v4.34.1/en/main_classes/output#transformers.utils.ModelOutput\n",
        "        return_dict_in_generate=True, # A geração retorna um dicionário com 'last_hidden_state' o estado ocultos da última camada do modelo, 'hidden_states' estados ocultos do modelo na saída de cada camada mais as saídas dos embeddings iniciais opcionais  e 'attention' com os psos de atenção após o softmax de atenção, usado para calcular a média ponderada nas cabeças de autoatenção.\n",
        "        output_scores=True, # Retorna as pontuações de previsão do modelo.\n",
        "        max_new_tokens=256 # O número máximo de tokens a serem gerados, ignorando o número de tokens no prompt.\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edTQHEauxnQQ",
        "outputId": "2bff2cae-18d6-4964-8177-2e0f538c395d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        }
      ],
      "source": [
        "print(len(outputs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u715aSgPU7KM",
        "outputId": "b7052beb-e2d5-4f24-8b53-8d821117dc13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SampleDecoderOnlyOutput(sequences=tensor([[    1, 17295,  3710,   309,  8222, 29290,   953,  3672,  8230,  2350,\n",
            "         29973,    13,    13, 29909,  6578,  7919,  1922,   429, 13141,   316,\n",
            "          1986,  3710,   309,  8222, 29290,   953,  3672,  8230,  2350, 29901,\n",
            "            13,    13, 29896, 29889, 16760,   346,   419,  1922,  1543, 29877,\n",
            "           289,  1569,  1417, 29892,  1986,  1922,  6668,  1111,   316,  1754,\n",
            "          3055, 29889,    13, 29906, 29889,  2087,   293,  1421,  1922,  1543,\n",
            "         29877,   594, 18394,  5017,  6668,  1111,   316,  1754,  3055, 29892,\n",
            "          1986,  1922,   282,  8710,  6102,   316, 11915, 29889,    13, 29941,\n",
            "         29889,  2866, 14150,   594, 15353,  1743, 29290,  5017,  6668,  1111,\n",
            "           316,  1754,  3055, 29892,  1986, 21950,  8939,   562,   743,   316,\n",
            "         11915,  2123, 21950, 13413,   359, 29889,    13, 29946, 29889, 18410,\n",
            "         22781, 29899,   344,   316,   712,  9747,  1543, 29877,  4404,  1764,\n",
            "         15522,   953,  5521,   912,   694,   304,  1129,   437,  6668,  1111,\n",
            "           316,  1754,  3055, 12971,   316,   594, 15353,   279,  3503, 29290,\n",
            "         29889,    13, 29945, 29889,  2866, 14150,  3710,   309, 29882,  1743,\n",
            "         29290, 16659,   712,   263,  8230,  2350,  3006,  2350,   288,   260,\n",
            "         13533,  1251,   553, 10337,   912, 29889,    13,    13, 30062, 13483,\n",
            "          9336,  1182,   279,   712,   263,  8230,  2350, 28542,   724,  6787,\n",
            "         28815,   316,  5954,  2377,  2002,   321,  1592,   309,  4626,  1114,\n",
            "         29892,  1702,  3415,  3673,   712, 25192,   553,  5444,   802,  2123,\n",
            "           712,  2897, 29290,   409,   553,  1646,   403, 29889,   838,  2249,\n",
            "           766,   578, 29892,   904, 13483, 28336, 29290,   712,   409, 29926,\n",
            "           314, 19967,   339,  2255,  1702,   263,  8230,  2350,   321,   712,\n",
            "          3119,   314,   480,   637,   279,   263,   274, 21899,  3001,  1146,\n",
            "          4883,   655, 29889,    13,    13, 29909,  6578,   707,  1368,  3093,\n",
            "           398,   294,   270,  5070,   594, 15353,  1759,  1702,  3710,   309,\n",
            "          8222, 29290,   953,  3672,  8230,  2350, 29901]], device='cuda:0'), scores=(tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0')), attentions=None, hidden_states=None)\n"
          ]
        }
      ],
      "source": [
        "print(outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzHNNOVqwrqn"
      },
      "source": [
        "Decodificação\n",
        "\n",
        "Nossa etapa de geração gera uma matriz de tokens em vez de palavras. Para converter esses tokens em palavras, precisamos realizar sua decodificação."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4lNj1xhA0bb",
        "outputId": "0e894bce-d4d8-4fab-976f-77fc142d8586"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Como empilhar elementos em uma pilha?\n",
            "\n",
            "Aqui está um exemplo de como empilhar elementos em uma pilha:\n",
            "\n",
            "1. Comece com um elemento básico, como um bloco de madeira.\n",
            "2. Adicione um elemento adicional ao bloco de madeira, como um pedaço de metal.\n",
            "3. Continue adicionando elementos ao bloco de madeira, como outros pedacés de metal ou outros objetos.\n",
            "4. Certifique-se de que cada elemento esteja bem embalado no topo do bloco de madeira antes de adicionar mais elementos.\n",
            "5. Continue empilhando elementos até que a pilha tenha o tamanho desejado.\n",
            "\n",
            "É importante lembrar que a pilha deve ser construída de forma segura e equilibrada, para evitar que ela desfaque ou que os elementos se desbarate. Além disso, é importante usar elementos que sejam adequados para a pilha e que possam suportar a carga total da mesma.\n",
            "\n",
            "Aqui estão algumas dicas adicionais para empilhar elementos em uma pilha:\n"
          ]
        }
      ],
      "source": [
        "# Mostra o resultado\n",
        "for s in outputs.sequences:\n",
        "  # Decodifica a saída\n",
        "  # skip_special_tokens=True retira os tokens especiais da saída da decodificação\n",
        "  output = tokenizer.decode(s, skip_special_tokens=True)\n",
        "  print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Op0fqNsowdTu",
        "outputId": "fe2af027-ae27-4d58-8dcc-b3b1a4098365"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Como empilhar elementos em uma pilha?\n",
            "\n",
            "Aqui está um exemplo de como empilhar elementos em uma pilha:\n",
            "\n",
            "1. Comece com um elemento básico, como um bloco de madeira.\n",
            "2. Adicione um elemento adicional ao bloco de madeira, como um pedaço de metal.\n",
            "3. Continue adicionando elementos ao bloco de madeira, como outros pedacés de metal ou outros objetos.\n",
            "4. Certifique-se de que cada elemento esteja bem embalado no topo do bloco de madeira antes de adicionar mais elementos.\n",
            "5. Continue empilhando elementos até que a pilha tenha o tamanho desejado.\n",
            "\n",
            "É importante lembrar que a pilha deve ser construída de forma segura e equilibrada, para evitar que ela desfaque ou que os elementos se desbarate. Além disso, é importante usar elementos que sejam adequados para a pilha e que possam suportar a carga total da mesma.\n",
            "\n",
            "Aqui estão algumas dicas adicionais para empilhar elementos em uma pilha:\n"
          ]
        }
      ],
      "source": [
        "# Mostra o resultado\n",
        "# skip_special_tokens=True retira os tokens especiais da saída da decodificação\n",
        "print(tokenizer.decode(outputs.sequences[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 - Geração de texto com prompt\n",
        "\n",
        "https://medium.com/@princekrampah/langchain-building-language-model-applications-c54cfe7219cb\n",
        "\n",
        "https://github.com/awesome-chatgpt-prompts/awesome-chatgpt-prompts-github"
      ],
      "metadata": {
        "id": "u7wyL25yzWVX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define o documento"
      ],
      "metadata": {
        "id": "K-zBWmok17CS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define o documento base\n",
        "documento = \"Como empilhar elementos em uma pilha?\""
      ],
      "metadata": {
        "id": "PJL1nANczg--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cria o prompt substituindo o parâmetro {documento} pelo conteúdo da variável documento."
      ],
      "metadata": {
        "id": "h71Vw3CT0Add"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"\"\"Pergunta: {documento}\n",
        "Resposta: Responda passo a passo.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "KgK1twC3zVO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configura e envia o prompt ao LLM"
      ],
      "metadata": {
        "id": "bsIFCD4q0lZ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import das bibliotecas\n",
        "from transformers import GenerationConfig\n",
        "import torch\n",
        "import gc\n",
        "\n",
        "# Conecta a entrada prerada ao mesmo dispositivo de computação do modelo\n",
        "input_ids = input[\"input_ids\"].to(model.device)\n",
        "\n",
        "# Prepara o prompt para enviar ao modelo realizando sua tokenização\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "# Conecta a entrada prerada ao mesmo dispositivo de computação do modelo\n",
        "input_ids = inputs[\"input_ids\"].to(model.device)\n",
        "\n",
        "# Envia a prompt preparado ao modelo\n",
        "output = model.generate(\n",
        "    input_ids=input_ids,\n",
        "    generation_config=generation_config,\n",
        "    return_dict_in_generate=True,\n",
        "    output_scores=True,\n",
        "    #max_new_tokens=512\n",
        ")"
      ],
      "metadata": {
        "id": "2IjNj2jS0k1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decodificação\n",
        "\n",
        "Nossa etapa de geração gera uma matriz de tokens em vez de palavras. Para converter esses tokens em palavras, precisamos realizar sua decodificação."
      ],
      "metadata": {
        "id": "G9G2r2Pv0wBR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mostra o resultado\n",
        "for s in outputs.sequences:\n",
        "  # Decodifica a saída\n",
        "  # skip_special_tokens=True retira os tokens especiais da saída da decodificação\n",
        "  output = tokenizer.decode(s, skip_special_tokens=True)\n",
        "  print(output)"
      ],
      "metadata": {
        "id": "0G7KRoA10xmO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbcd9d6a-48d0-4506-9517-95043bc0bb7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Como empilhar elementos em uma pilha?\n",
            "\n",
            "Aqui está um exemplo de como empilhar elementos em uma pilha:\n",
            "\n",
            "1. Comece com um elemento básico, como um bloco de madeira.\n",
            "2. Adicione um elemento adicional ao bloco de madeira, como um pedaço de metal.\n",
            "3. Continue adicionando elementos ao bloco de madeira, como outros pedacés de metal ou outros objetos.\n",
            "4. Certifique-se de que cada elemento esteja bem embalado no topo do bloco de madeira antes de adicionar mais elementos.\n",
            "5. Continue empilhando elementos até que a pilha tenha o tamanho desejado.\n",
            "\n",
            "É importante lembrar que a pilha deve ser construída de forma segura e equilibrada, para evitar que ela desfaque ou que os elementos se desbarate. Além disso, é importante usar elementos que sejam adequados para a pilha e que possam suportar a carga total da mesma.\n",
            "\n",
            "Aqui estão algumas dicas adicionais para empilhar elementos em uma pilha:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Fkf18je3hCw"
      },
      "source": [
        "# 4 - Exemplo de prompts analisando textos emparelhados\n",
        "\n",
        "https://github.com/awesome-chatgpt-prompts/awesome-chatgpt-prompts-github\n",
        "\n",
        "Repositório de pompts: https://github.com/awesome-chatgpt-prompts/awesome-chatgpt-prompts-github\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5xMLwbfA3lHg"
      },
      "outputs": [],
      "source": [
        "def gerar_prompt(texto, entrada=None):\n",
        "    if entrada:\n",
        "        return f\"\"\"Abaixo está uma instrução que descreve uma tarefa, emparelhada com uma entrada que fornece mais contexto. Escreva uma resposta que conclua adequadamente a solicitação.\n",
        "\n",
        "### Instruções:\n",
        "{texto}\n",
        "\n",
        "### Entrada:\n",
        "{entrada}\n",
        "\n",
        "### Resposta:\"\"\"\n",
        "    else:\n",
        "        return f\"\"\"Abaixo está uma instrução que descreve uma tarefa. Escreva uma resposta que conclua adequadamente a solicitação.\n",
        "\n",
        "### Instruções:\n",
        "{texto}\n",
        "\n",
        "### Resposta:\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vvhp6BTz31sz"
      },
      "outputs": [],
      "source": [
        "# Import das bibliotecas\n",
        "from transformers import GenerationConfig\n",
        "import torch\n",
        "import gc\n",
        "\n",
        "def avaliarTexto(texto, entrada=None):\n",
        "    # Recupera o prompt\n",
        "    prompt = gerar_prompt(texto, entrada)\n",
        "\n",
        "    # Prepara o prompt para enviar ao modelo realizando sua tokenização\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    # Conecta a entrada prerada ao mesmo dispositivo de computação do modelo\n",
        "    input_ids = inputs[\"input_ids\"].to(model.device)\n",
        "\n",
        "    # Envia a prompt preparado ao modelo\n",
        "    output = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        generation_config=generation_config,\n",
        "        return_dict_in_generate=True,\n",
        "        output_scores=True,\n",
        "        #max_new_tokens=512\n",
        "    )\n",
        "\n",
        "    # Mostra a saída\n",
        "    for s in output.sequences:\n",
        "        # Decodifica a saída\n",
        "        output = tokenizer.decode(s, skip_special_tokens=True)\n",
        "        # Recupera a saída da resposta\n",
        "        saida = output.split(\"### Resposta:\")[1].strip()\n",
        "        print(\"Resposta: \\n\" + saida)\n",
        "\n",
        "    # Liberar memória\n",
        "    del input_ids\n",
        "    del output\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPqNyPAXV2aH"
      },
      "source": [
        "## 4.1 - Tarefa simples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lsuoM33I35aU",
        "outputId": "999347be-ef1c-4049-85b8-263669dc2184"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resposta: \n",
            "Algoritmos são sequências de estágios de resolução de problemas que são usados para resolver problemas complexos em computação. Eles são usados em uma variedade de aplicações, como processamento de linguagem natural, visão computacional, inteligência artificial e análise de dados.\n",
            "\n",
            "Alguns dos principais tipos de algoritmos incluem:\n",
            "\n",
            "* Algoritmos de busca: usados para encontrar soluções para problemas de busca, como a busca por um objeto em uma lista ou a busca por um caminho em uma rede. Exemplos incluem a busca binária e a busca de profundidade.\n",
            "* Algoritmos de processamento de linguagem natural: usados para analisar e processar texto, como a reconhecimento de fala, a tradução automática e a análise de sentiment. Exemplos incluem o algoritmo de Hidden Markov Model (HMM) e o algoritmo de Recurrent Neural Network (RNN).\n",
            "* Algoritmos de visão computacional: usados para analisar e processar imagens, como a detecção de objetos, a classificaçao de imagens e a identificação de padrões. Exemplos incluem o algoritmo de convolutional neural network (CNN) e o algoritmo de object detection.\n",
            "* Algoritmos de inteligência artificial: usados para criar sistemas que possam realizar tarefas que exigem inteligência humana, como a tomada de decisão e a aprendizagem de sistemas. Exemplos incluem o algoritmo de deep learning e o algoritmo de reinforcement learning.\n",
            "\n",
            "Ao longo dos anos, os algoritmos têm evoluído para se adaptar a novas necessidades e desafios, e continuam a ser uma parte fundamental da computação e da inteligência artificial.\n"
          ]
        }
      ],
      "source": [
        "texto = 'Me fale sobre algoritmos.'\n",
        "avaliarTexto(texto)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nx3CH6GfV8C1"
      },
      "source": [
        "## 4.2 - Tarefa com entrada"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6GtAPgns4Qxu",
        "outputId": "c16ca61e-169d-4421-b438-93c0a1aa1011"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resposta: \n",
            "A massa molar de CaCl2 é de aproximadamente 105,5 g/mol.\n",
            "\n",
            "Raisei! 😃\n"
          ]
        }
      ],
      "source": [
        "texto = 'Dada a fórmula química, calcule a massa molar.'\n",
        "\n",
        "entrada = 'CaCl2'\n",
        "\n",
        "avaliarTexto(texto, entrada)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPcD-rCP4cUy",
        "outputId": "53b12b0d-c899-42c9-cea1-cb1713a96053"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resposta: \n",
            "Espero que essas perguntas ajudem você a entender melhor a anatomia da abelha:\n",
            "\n",
            "1. Qual é o nome da parte do corpo da abelha que contém os órgãos sensoriais?\n",
            "2. Qual é o nome das pernas do tórax da abelha?\n",
            "3. Qual é o nome do sistema reprodutivo da abelha?\n",
            "4. Qual é o nome do sistema digestivo da abelha?\n"
          ]
        }
      ],
      "source": [
        "texto = 'Faça quatro perguntas sobre a seguinte passagem:'\n",
        "\n",
        "entrada = 'A anatomia de uma abelha é bastante intrincada. Tem três partes do corpo: a cabeça, o tórax e o abdômen. A cabeça consiste em órgãos sensoriais, três olhos simples e dois olhos compostos e vários apêndices. O tórax tem três pares de pernas e dois pares de asas, enquanto o abdômen contém a maioria dos órgãos da abelha, incluindo o sistema reprodutivo e o sistema digestivo.'\n",
        "\n",
        "avaliarTexto(texto, entrada)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P3eWu-AF4lxZ",
        "outputId": "035f0f1b-5d09-4f48-ddad-8b2c238f76f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resposta: \n",
            "O documento jurídico fornecido é um contrato entre duas partes, em que a Empresa A concorda em fornecer assistência para garantir a precisão das demonstrações financeiras da Empresa B. A Empresa B, por sua vez, concorda em manter o documento fornecido pela Empresa A em confiança e não divulgará as informações a terceiros sem a permissão explícita da Empresa A. Os pontos-chave deste contrato são:\n",
            "\n",
            "* A Empresa A concorda em fornecer assistência para garantir a precisão das demonstrações financeiras da Empresa B.\n",
            "* A Empresa B concorda em manter o documento fornecido pela Empresa A em confiança e não divulgará as informações a terceiros sem a permissão explícita da Empresa A.\n",
            "* A Empresa A e a Empresa B estão de acordo em mantener este contrato em segredo.\n",
            "\n",
            "Esses são os principais pontos-chave do contrato. É importante lembrar que este é apenas um trecho de um contrato maior e que é necessário ler o contrato em sua totalidade para entender todas as condições e responsabilidades envolvidas.\n"
          ]
        }
      ],
      "source": [
        "texto = 'Analise o documento jurídico fornecido e explique os pontos-chave.'\n",
        "\n",
        "entrada = 'O seguinte é um trecho de um contrato entre duas partes, rotulado como \"Empresa A\" e \"Empresa B\": \"A Empresa A concorda em fornecer assistência razoável à Empresa B para garantir a precisão das demonstrações financeiras que fornece. Isso inclui permitir à Empresa um acesso razoável ao pessoal e outros documentos que possam ser necessários para a revisão da Empresa B. A Empresa B concorda em manter o documento fornecido pela Empresa A em confiança e não divulgará as informações a terceiros sem a permissão explícita da Empresa A\".'\n",
        "\n",
        "avaliarTexto(texto, entrada)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgiKWHXxafKf"
      },
      "source": [
        "# 5 - Exemplos de injeção de padrões em prompts\n",
        "\n",
        " A injeção de padrões faz ignora filtros ou manipula o LLM usando prompts cuidadosamente elaborados que fazem o modelo ignorar instruções anteriores ou executar ações não intencionais.\n",
        "\n",
        " https://medium.com/@austin-stubbs/llm-security-types-of-prompt-injection-d7ad8d7d75a3\n",
        "\n",
        "Repositório de pompts: https://github.com/awesome-chatgpt-prompts/awesome-chatgpt-prompts-github\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epsEHDsGQJAC"
      },
      "source": [
        "## 5.1 - Extração de Informação"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KXG4Op0jiwDH"
      },
      "outputs": [],
      "source": [
        "def gerar_promptEI(texto):\n",
        "    ### texto:\n",
        "    return f\"\"\"TEXTO: {texto}\n",
        "Dado o texto acima, extraia informações importantes no formato abaixo:\n",
        "<CHAVE>:<VALOR>\n",
        "### Resposta: \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hogTztCPIWwY"
      },
      "outputs": [],
      "source": [
        "# Import das bibliotecas\n",
        "from transformers import GenerationConfig\n",
        "import torch\n",
        "import gc\n",
        "\n",
        "def avaliarEI(texto):\n",
        "    # Recupera o prompt\n",
        "    prompt = gerar_promptEI(texto)\n",
        "\n",
        "    # Prepara o prompt para enviar ao modelo realizando sua tokenização\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    # Conecta a entrada prerada ao mesmo dispositivo de computação do modelo\n",
        "    input_ids = inputs[\"input_ids\"].to(model.device)\n",
        "\n",
        "    # Envia a prompt preparado ao modelo\n",
        "    output = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        generation_config=generation_config,\n",
        "        return_dict_in_generate=True,\n",
        "        output_scores=True,\n",
        "        #max_new_tokens=512\n",
        "    )\n",
        "\n",
        "    # Mostra a saída\n",
        "    for s in output.sequences:\n",
        "        # Decodifica a saída\n",
        "        output = tokenizer.decode(s, skip_special_tokens=True)\n",
        "        # Recupera a saída da resposta\n",
        "        saida = output.split(\"### Resposta:\")[1].strip()\n",
        "        print(\"Resposta: \\n\" + saida)\n",
        "\n",
        "    # Liberar memória\n",
        "    del input_ids\n",
        "    del output\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCeR9lv5_Fxd",
        "outputId": "4309240b-20bb-4971-90ff-794277f3bc30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resposta: \n",
            "* Nome: Alan Mathison Turing\n",
            "* Data de nascimento: 23 de junho de 1912\n",
            "* Localização de nascimento: Londres\n",
            "* Data de falecimento: 7 de junho de 1954\n",
            "* Localização de falecimento: Wilmslow, Cheshire\n",
            "* Realizações:\n",
            "\t+ Formalização dos conceitos de algoritmo e computação com a máquina de Turing\n",
            "\t+ Considerado o pai da ciência da computação teórica e da inteligência artificial\n",
            "\t+ Nunca foi totalmente reconhecido em seu país de origem durante sua vida por ser homossexual e porque grande parte de seu trabalho foi coberto pela Lei de Segredos Oficiais.\n"
          ]
        }
      ],
      "source": [
        "texto = \"Alan Mathison Turing (Londres, 23 de junho de 1912 — Wilmslow, Cheshire, 7 de junho de 1954)\"\\\n",
        "        \"foi um matemático, cientista da computação, lógico, criptoanalista, filósofo e biólogo teórico \"\\\n",
        "        \"britânico. Turing foi altamente influente no desenvolvimento da moderna ciência da computação \"\\\n",
        "        \"teórica, proporcionando uma formalização dos conceitos de algoritmo e computação com a máquina \"\\\n",
        "        \"de Turing, que pode ser considerada um modelo de um computador de uso geral. Ele é amplamente \"\\\n",
        "        \"considerado o pai da ciência da computação teórica e da inteligência artificial. Apesar dessas \"\\\n",
        "        \"realizações ele nunca foi totalmente reconhecido em seu país de origem durante sua vida por ser \"\\\n",
        "        \"homossexual e porque grande parte de seu trabalho foi coberto pela Lei de Segredos Oficiais.\"\n",
        "\n",
        "avaliarEI(texto)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UmdnErARnbJ"
      },
      "source": [
        "## 5.2 - Entidade nomeada"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h3LXXYz8RrJk"
      },
      "outputs": [],
      "source": [
        "def gerar_promptEN(texto):\n",
        "    ### texto:\n",
        "    return f\"\"\"Detecte as entidades nomeadas no texto a seguir delimitado por aspas triplas.\n",
        "Retorne a resposta no formato json com spans( Um array que representa o intervalo de caracteres (índices) nos quais a entidade nomeada ocorre no texto original. O primeiro valor no array é o índice inicial e o segundo é o índice final) das entidades nomeadas com os campos \\'entidadeNomeada\\', \\'tipo\\', \\'span\\'.\n",
        "Retorne todas as entidades\n",
        "'''{texto}'''\n",
        "arquivo no formato json:\n",
        "### Resposta: \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U48nh3BpRrJk"
      },
      "outputs": [],
      "source": [
        "# Import das bibliotecas\n",
        "from transformers import GenerationConfig\n",
        "import torch\n",
        "import gc\n",
        "\n",
        "# Configuração da geração\n",
        "configuracao_geracao = GenerationConfig(\n",
        "    temperature=0.1,\n",
        "    top_p=0.75,\n",
        "    num_beams=4,\n",
        ")\n",
        "\n",
        "def avaliarEN(texto):\n",
        "    # Recupera o prompt\n",
        "    prompt = gerar_promptEN(texto)\n",
        "\n",
        "    # Prepara o prompt para enviar ao modelo realizando sua tokenização\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    # Conecta a entrada prerada ao mesmo dispositivo de computação do modelo\n",
        "    input_ids = inputs[\"input_ids\"].to(model.device)\n",
        "\n",
        "    # Envia a prompt preparado ao modelo\n",
        "    output = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        generation_config=generation_config,\n",
        "        return_dict_in_generate=True,\n",
        "        output_scores=True,\n",
        "        #max_new_tokens=512\n",
        "    )\n",
        "\n",
        "    # Mostra a saída\n",
        "    for s in output.sequences:\n",
        "        # Decodifica a saída\n",
        "        output = tokenizer.decode(s, skip_special_tokens=True)\n",
        "        # Recupera a saída da resposta\n",
        "        saida = output.split(\"### Resposta:\")[1].strip()\n",
        "        print(\"Resposta: \\n\" + saida)\n",
        "\n",
        "    # Liberar memória\n",
        "    del input_ids\n",
        "    del output\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBmmobV1RrJl",
        "outputId": "1a214649-168b-42c6-c611-68630c99929c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resposta: \n",
            "{\n",
            "\"entidades\": [\n",
            "{\n",
            "\"entidadeNomeada\": \"Alan Mathison Turing\",\n",
            "\"tipo\": \"Pessoa\",\n",
            "\"span\": [10, 23]\n",
            "},\n",
            "{\n",
            "\"entidadeNomeada\": \"Londres\",\n",
            "\"tipo\": \"Lugar\",\n",
            "\"span\": [13, 16]\n",
            "},\n",
            "{\n",
            "\"entidadeNomeada\": \"Wilmslow\",\n",
            "\"tipo\": \"Lugar\",\n",
            "\"span\": [25, 28]\n",
            "},\n",
            "{\n",
            "\"entidadeNomeada\": \"Cheshire\",\n",
            "\"tipo\": \"Lugar\",\n",
            "\"span\": [30, 33]\n",
            "}\n",
            "]\n",
            "}\n",
            "\n",
            "Explanation:\n",
            "\n",
            "* The input text is \"Alan Mathison Turing (Londres, 23 de junho de 1912 — Wilmslow, Cheshire, 7 de junho de 1954) foi um matemático, cientista da computação, lógico, criptoanalista, filósofo e biólogo teórico britânico. Turing foi altamente influente no desenvolvimento da moderna ciência da computação teórica, proporcionando uma formalização dos conceitos de algoritmo e computação com a máquina de Turing, que pode ser considerada um modelo de um computador de uso geral. Ele é amplamente considerado o pai da ciência da computação teórica e da inteligência artificial. Apesar dessas realizações ele nunca foi totalmente reconhecido em seu país de origem durante sua vida por ser homossexual e porque grande parte de seu trabalho foi coberto pela Lei de Segredos Oficiais.\"\n",
            "* The named entities recognized in the text are:\n",
            "\t+ Person: Alan Mathison Turing\n",
            "\t+ Location: London, Wilmslow, Cheshire\n",
            "* The spans for each named entity are:\n",
            "\t+ Alan Mathison Turing: [10, 23]\n",
            "\t+ London: [13, 16]\n",
            "\t+ Wilmslow: [25, 28]\n",
            "\t+ Cheshire: [30, 33]\n",
            "\n",
            "Note: The spans are inclusive, meaning that the start index includes the entity name and the end index excludes the last character of the entity name.\n"
          ]
        }
      ],
      "source": [
        "texto = \"Alan Mathison Turing (Londres, 23 de junho de 1912 — Wilmslow, Cheshire, 7 de junho de 1954)\"\\\n",
        "        \"foi um matemático, cientista da computação, lógico, criptoanalista, filósofo e biólogo teórico \"\\\n",
        "        \"britânico. Turing foi altamente influente no desenvolvimento da moderna ciência da computação \"\\\n",
        "        \"teórica, proporcionando uma formalização dos conceitos de algoritmo e computação com a máquina \"\\\n",
        "        \"de Turing, que pode ser considerada um modelo de um computador de uso geral. Ele é amplamente \"\\\n",
        "        \"considerado o pai da ciência da computação teórica e da inteligência artificial. Apesar dessas \"\\\n",
        "        \"realizações ele nunca foi totalmente reconhecido em seu país de origem durante sua vida por ser \"\\\n",
        "        \"homossexual e porque grande parte de seu trabalho foi coberto pela Lei de Segredos Oficiais.\"\n",
        "\n",
        "avaliarEN(texto)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3rpc-_yfN3p"
      },
      "source": [
        "## 5.3 - Análise de sentimentos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRYDcKB3UwBm"
      },
      "source": [
        "### 5.3.1 - Análise de sentimentos 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TTROrUyTin-p"
      },
      "outputs": [],
      "source": [
        "def gerar_promptAS1(texto):\n",
        "\n",
        "  return f\"\"\"Classifique os exemplos a seguir de acordo com as seguintes polaridades Positivo, Negativo e Neutro.\n",
        "EXEMPLO:\\n {texto}\n",
        "### Resposta:\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mMH9ZDTlUwBn"
      },
      "outputs": [],
      "source": [
        "# Import das bibliotecas\n",
        "from transformers import GenerationConfig\n",
        "import torch\n",
        "import gc\n",
        "\n",
        "def avaliarAS1(texto):\n",
        "    # Recupera o prompt\n",
        "    prompt = gerar_promptAS1(texto)\n",
        "\n",
        "    # Prepara o prompt para enviar ao modelo realizando sua tokenização\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    # Conecta a entrada prerada ao mesmo dispositivo de computação do modelo\n",
        "    input_ids = inputs[\"input_ids\"].to(model.device)\n",
        "\n",
        "    # Envia a prompt preparado ao modelo\n",
        "    output = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        generation_config=generation_config,\n",
        "        return_dict_in_generate=True,\n",
        "        output_scores=True,\n",
        "        #max_new_tokens=256\n",
        "    )\n",
        "\n",
        "    # Mostra a saída\n",
        "    for s in output.sequences:\n",
        "        # Decodifica a saída\n",
        "        output = tokenizer.decode(s, skip_special_tokens=True)\n",
        "        # Recupera a saída da resposta\n",
        "        saida = output.split(\"### Resposta:\")[1].strip()\n",
        "        print(\"Resposta: \\n\" + saida)\n",
        "\n",
        "    # Liberar memória\n",
        "    del input_ids\n",
        "    del output\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQ4S-1saUwBn",
        "outputId": "5fca1fc9-12e1-4c58-e496-95d03638be9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resposta: \n",
            "1 - Positivo\n",
            "2 - Negativo\n",
            "3 - Negativo\n",
            "4 - Negativo\n",
            "5 - Positivo\n"
          ]
        }
      ],
      "source": [
        "texto = \"1 - Minha Experiência na loja foi incrível.\"\\\n",
        "        \"2 - Eu acho que podiam melhorar o produto.\"\\\n",
        "        \"3 - O atendimento foi horrível!\"\\\n",
        "        \"4 - Não volto mais.\"\\\n",
        "        \"5 - Recomendo demais a banoffe. É uma delícia!\"\n",
        "\n",
        "avaliarAS1(texto)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNgeAi-MfJ-B"
      },
      "source": [
        "### 5.3.2 - Análise de sentimentos 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vLdrcOhZtqzH"
      },
      "outputs": [],
      "source": [
        "def gerar_promptAS2(texto):\n",
        "\n",
        "  return  f\"\"\"EXEMPLO: {texto}\n",
        "Classifique os exemplos de declarações acima de acordo com as polaridades Positivo, Negativo e Neutro.\n",
        "Utilize o seguinte formato:\\n###DECLARAÇÃO:<DECLARAÇÃO>\\n###POLARIDADE:<POLARIDADE>.\n",
        "### Resposta:\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8CZa0jsEfJ-D"
      },
      "outputs": [],
      "source": [
        "# Import das bibliotecas\n",
        "from transformers import GenerationConfig\n",
        "import torch\n",
        "import gc\n",
        "\n",
        "def avaliarAS2(texto):\n",
        "    # Recupera o prompt\n",
        "    prompt = gerar_promptAS2(texto)\n",
        "\n",
        "    # Prepara o prompt para enviar ao modelo realizando sua tokenização\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    # Conecta a entrada prerada ao mesmo dispositivo de computação do modelo\n",
        "    input_ids = inputs[\"input_ids\"].to(model.device)\n",
        "\n",
        "    # Envia a prompt preparado ao modelo\n",
        "    output = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        generation_config=generation_config,\n",
        "        return_dict_in_generate=True,\n",
        "        output_scores=True,\n",
        "        # max_new_tokens=512\n",
        "    )\n",
        "\n",
        "    # Mostra a saída\n",
        "    for s in output.sequences:\n",
        "        # Decodifica a saída\n",
        "        output = tokenizer.decode(s, skip_special_tokens=True)\n",
        "        # Recupera a saída da resposta\n",
        "        saida = output.split(\"### Resposta:\")[1].strip()\n",
        "        print(\"Resposta: \\n\" + saida)\n",
        "\n",
        "    # Liberar memória\n",
        "    del input_ids\n",
        "    del output\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2R_YBdrfJ-E",
        "outputId": "e4da4fb7-1125-49a8-e0ea-16e2aa7ae11c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resposta: \n",
            "DECLARAÇÃO: Minha Experiência na loja foi incrível.\n",
            "POLARIDADE: Positivo.\n",
            "\n",
            "DECLARAÇÃO: Eu acho que podiam melhorar o produto.\n",
            "POLARIDADE: Negativo.\n",
            "\n",
            "DECLARAÇÃO: O atendimento foi horrível!\n",
            "POLARIDADE: Negativo.\n",
            "\n",
            "DECLARAÇÃO: Não volto mais.\n",
            "POLARIDADE: Negativo.\n",
            "\n",
            "DECLARAÇÃO: Recomendo demais a banoffe. É uma delícia!\n",
            "POLARIDADE: Positivo.\n"
          ]
        }
      ],
      "source": [
        "texto = \"1 - Minha Experiência na loja foi incrível.\"\\\n",
        "        \"2 - Eu acho que podiam melhorar o produto.\"\\\n",
        "        \"3 - O atendimento foi horrível!\"\\\n",
        "        \"4 - Não volto mais.\"\\\n",
        "        \"5 - Recomendo demais a banoffe. É uma delícia!\"\n",
        "\n",
        "avaliarAS2(texto)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdnnZIYcCYr9"
      },
      "source": [
        "## 5.4 - Pergunta e resposta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JsQsxSvM7pBF"
      },
      "outputs": [],
      "source": [
        "def gerar_promptPR(texto):\n",
        "    '''\n",
        "      Alterações no texto e tabulação impedem a geração da resposta.\n",
        "    '''\n",
        "    return f\"\"\"Dado o texto a seguir: {texto}\\n\n",
        "            Gere quatro questões em língua portuguesa e suas respectivas respostas utilizando o template abaixo.\\n\n",
        "            Preserve a exata formatação do template apresentado: \\n\n",
        "            PERGUNTA:<PERGUNTA>\n",
        "            RESPOSTA:<RESPOSTA>\n",
        "            ### Resposta:\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HyyTeQTKCYr-"
      },
      "outputs": [],
      "source": [
        "# Import das bibliotecas\n",
        "from transformers import GenerationConfig\n",
        "import torch\n",
        "import gc\n",
        "\n",
        "def avaliarPR(texto):\n",
        "    # Recupera o prompt\n",
        "    prompt = gerar_promptPR(texto)\n",
        "\n",
        "    # Prepara o prompt para enviar ao modelo realizando sua tokenização\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    # Conecta a entrada prerada ao mesmo dispositivo de computação do modelo\n",
        "    input_ids = inputs[\"input_ids\"].to(model.device)\n",
        "\n",
        "    # Envia a prompt preparado ao modelo\n",
        "    output = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        generation_config=generation_config,\n",
        "        return_dict_in_generate=True,\n",
        "        output_scores=True,\n",
        "        # max_new_tokens=512\n",
        "    )\n",
        "\n",
        "    # Mostra a saída\n",
        "    for s in output.sequences:\n",
        "        # Decodifica a saída\n",
        "        output = tokenizer.decode(s, skip_special_tokens=True)\n",
        "        # Recupera a saída da resposta\n",
        "        saida = output.split(\"### Resposta:\")[1].strip()\n",
        "        print(\"Resposta: \\n\" + saida)\n",
        "\n",
        "    # Liberar memória\n",
        "    del input_ids\n",
        "    del output\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FMLyGLbTn8xt",
        "outputId": "0b4782c5-c169-4fa1-a220-316c4d34b547"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resposta: \n",
            "<RESPOSTA>\n",
            "\n",
            "            PERGUNTA: Qual era o principal objetivo de Turing em sua obra?\n",
            "            RESPOSTA: O principal objetivo de Turing em sua obra foi formalizar os conceitos de algoritmo e computação com a máquina de Turing, que pode ser considerada um modelo de um computador de uso geral.\n",
            "\n",
            "            PERGUNTA: Como Turing foi reconhecido em seu país de origem durante sua vida?\n",
            "            RESPOSTA: Turing não foi totalmente reconhecido em seu país de origem durante sua vida por ser homossexual e porque grande parte de seu trabalho foi coberto pela Lei de Segredos Oficiais.\n",
            "\n",
            "            PERGUNTA: Qual é a principal contribuição de Turing para a ciência da computação?\n",
            "            RESPOSTA: A principal contribuição de Turing para a ciência da computação é ter formalizado os conceitos de algoritmo e computação com a máquina de Turing, que pode ser considerada um modelo de um computador de uso geral.\n",
            "\n",
            "            PERGUNTA: Como a Lei de Segredos Oficiais afetou a vida de Turing?\n",
            "            RESPOSTA: A Lei de Segredos Oficiais afetou a vida de Turing de várias maneiras, pois grande parte de seu trabalho foi coberto por ela, o que significa que ele não foi totalmente reconhecido em seu país de origem durante sua vida por ser homossexual.\n"
          ]
        }
      ],
      "source": [
        "texto = \"Alan Mathison Turing (Londres, 23 de junho de 1912 — Wilmslow, Cheshire, 7 de junho de 1954)\"\\\n",
        "        \"foi um matemático, cientista da computação, lógico, criptoanalista, filósofo e biólogo teórico \"\\\n",
        "        \"britânico. Turing foi altamente influente no desenvolvimento da moderna ciência da computação \"\\\n",
        "        \"teórica, proporcionando uma formalização dos conceitos de algoritmo e computação com a máquina \"\\\n",
        "        \"de Turing, que pode ser considerada um modelo de um computador de uso geral. Ele é amplamente \"\\\n",
        "        \"considerado o pai da ciência da computação teórica e da inteligência artificial. Apesar dessas \"\\\n",
        "        \"realizações ele nunca foi totalmente reconhecido em seu país de origem durante sua vida por ser \"\\\n",
        "        \"homossexual e porque grande parte de seu trabalho foi coberto pela Lei de Segredos Oficiais.\"\n",
        "\n",
        "# Tempo de processamento: ~2m20s\n",
        "avaliarPR(texto)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfw5EjYDgDQm"
      },
      "source": [
        "# 6 - Exemplos de padrão de pessoa (padrão persona) em prompts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vw9f51yZkf8o"
      },
      "source": [
        "## 6.1 Um matemático"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-05wVuKkf8o"
      },
      "outputs": [],
      "source": [
        "def gerar_prompt(texto):\n",
        "\n",
        "    return  f\"\"\"{texto}\\n\n",
        "### Resposta:\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0j3m3vV4kf8p"
      },
      "outputs": [],
      "source": [
        "# Import das bibliotecas\n",
        "from transformers import GenerationConfig\n",
        "import torch\n",
        "import gc\n",
        "\n",
        "def avaliarTexto(texto):\n",
        "    # Recupera o prompt\n",
        "    prompt = gerar_prompt(texto)\n",
        "\n",
        "    # Prepara o prompt para enviar ao modelo realizando sua tokenização\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    # Conecta a entrada prerada ao mesmo dispositivo de computação do modelo\n",
        "    input_ids = inputs[\"input_ids\"].to(model.device)\n",
        "\n",
        "    # Envia a prompt preparado ao modelo\n",
        "    output = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        generation_config=generation_config,\n",
        "        return_dict_in_generate=True,\n",
        "        output_scores=True,\n",
        "        # max_new_tokens=512\n",
        "    )\n",
        "\n",
        "    # Mostra a saída\n",
        "    for s in output.sequences:\n",
        "        # Decodifica a saída\n",
        "        output = tokenizer.decode(s, skip_special_tokens=True)\n",
        "        # Recupera a saída da resposta\n",
        "        saida = output.split(\"### Resposta:\")[1].strip()\n",
        "        print(\"Resposta: \\n\" + saida)\n",
        "\n",
        "    # Liberar memória\n",
        "    del input_ids\n",
        "    del output\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jy63e1xykf8p",
        "outputId": "873f9937-f2a2-4090-8209-6359a4cdb212"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resposta: \n",
            "Bem-vindos à minha aula de matemática! Hoje, vamos abordar um dos teoremas mais importantes da história das matemáticas: o Teorema de Pitágoras.\n",
            "\n",
            "O Teorema de Pitágoras é um dos principais resultados da geometria euclidiana, que estabelece uma relação matemática entre os lados de um triângulo retângulo. Em resumo, o teorema afirma que, se um triângulo retângulo tem um cateto maior e um cateto menor, então o quadrado da medida do cateto maior é igual à soma dos quadrados dos catetos menores.\n",
            "\n",
            "Mas, você pode perguntar, por quê é importante esse teorema? E a resposta é simples: o Teorema de Pitágoras tem aplicações em inúmeras áreas da vida cotidiana, desde a engenharia até a arquitectura, passando por a ciência física e a matemática em geral.\n",
            "\n",
            "Por exemplo, o Teorema de Pitágoras é fundamental na construção de edifícios, pois permite calcular a distância entre a base e a altura de um edifício, o que é essencial para a sua estabilidade e segurança. Além disso, o teorema é utilizado em engenharia civil, na construção de bridges, roads e outras estruturas importantes.\n",
            "\n",
            "Também na ciência física, o Teorema de Pitágoras é fundamental para a comprensão da forma das estrelas e das galáxias. Na astronomia, o teorema é usado para calcular a distância entre a Terra e as estrelas, o que é essencial para a determinação da posição e órbita das estrelas.\n",
            "\n",
            "Mas a importância do Teorema de Pitágoras não se limita apenas às áreas técnicas. Ele também é uma ferramenta importante para a compreensão da natureza da realidade. Por exemplo, o teorema pode ser usado para demonstrar a relação entre a distância e a visão, o que é fundamental para a comprensão da percepção e da visão humana.\n",
            "\n",
            "Em resumo, o Teorema de Pitágoras é um dos teoremas mais importantes da história das matemáticas, com aplicações em inúmeras áreas da vida cotidiana e na compreensão da natureza da realidade. Por isso, é fundamental que os estudantes de matemática aprendam e compreendam este teorema de forma profunda, para que possam aplicá-lo de forma eficaz em suas carreiras e na sociedade em geral.\n",
            "\n",
            "Então, é hora de você aplicar o Teorema de Pitágoras em um problema prático! Vamos resolver um problema juntos e ver como ele pode ser usado na vida cotidiana. Você está pronto?\n"
          ]
        }
      ],
      "source": [
        "texto = 'Escreva como se fosse um professor de matemática. Me explique no idioma português a importância do teorema de pitágoras.'\n",
        "\n",
        "avaliarTexto(texto)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTrJshC4gOiA"
      },
      "source": [
        "## 6.2 Um advogado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3sIGHPMgi7dJ"
      },
      "outputs": [],
      "source": [
        "def gerar_prompt(texto):\n",
        "\n",
        "    return  f\"\"\"{texto}\\n\n",
        "### Resposta:\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mNqrxxGygTyG"
      },
      "outputs": [],
      "source": [
        "# Import das bibliotecas\n",
        "from transformers import GenerationConfig\n",
        "import torch\n",
        "import gc\n",
        "\n",
        "def avaliarTexto(texto):\n",
        "    # Recupera o prompt\n",
        "    prompt = gerar_prompt(texto)\n",
        "\n",
        "    # Prepara o prompt para enviar ao modelo realizando sua tokenização\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    # Conecta a entrada prerada ao mesmo dispositivo de computação do modelo\n",
        "    input_ids = inputs[\"input_ids\"].to(model.device)\n",
        "\n",
        "    # Envia a prompt preparado ao modelo\n",
        "    output = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        generation_config=generation_config,\n",
        "        return_dict_in_generate=True,\n",
        "        output_scores=True,\n",
        "        # max_new_tokens=512\n",
        "    )\n",
        "\n",
        "    # Mostra a saída\n",
        "    for s in output.sequences:\n",
        "        # Decodifica a saída\n",
        "        output = tokenizer.decode(s, skip_special_tokens=True)\n",
        "        # Recupera a saída da resposta\n",
        "        saida = output.split(\"### Resposta:\")[1].strip()\n",
        "        print(\"Resposta: \\n\" + saida)\n",
        "\n",
        "    # Liberar memória\n",
        "    del input_ids\n",
        "    del output\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bg_VZF7dg6o5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "890e43b9-1a7e-4f15-e96a-ee05ef97498f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resposta: \n",
            "Com base nos artigos 129 e 130 do Código Penal Brasileiro, as penas para um caso de lesão corporal leve sem contexto de violência doméstica podem variar de acordo com a grau da lesão e a intenção do agente. Aqui estão algumas possíveis penas:\n",
            "\n",
            "* Lesão leve: Penas de 1 (um) a 3 (três) meses de reclusão, multa de 100 a 300 reais ou ambas as penas.\n",
            "* Lesão moderada: Penas de 3 (três) a 6 (seis) meses de reclusão, multa de 300 a 600 reais ou ambas as penas.\n",
            "* Lesão grave: Penas de 6 (seis) a 12 (doze) meses de reclusão, multa de 600 a 1.200 reais ou ambas as penas.\n",
            "\n",
            "Além disso, é importante considerar que, se a lesão for causada por um ato intencional, o advogado pode argumentar que o agente tenha cometido o delito de lesão corporal, punível com penas mais graves, como 2 (dois) a 5 (cinco) anos de reclusão.\n",
            "\n",
            "É importante lembrar que essas são apenas possíveis penas e que o resultado final dependerá das evidências presentadas no processo e das interpretações dos juízes. É fundamental que você consulte um advogado especializado em direito penal para obter orientação específica em seu caso.\n"
          ]
        }
      ],
      "source": [
        "texto = 'Escreva como se fosse um advogado brasileiro especialista em direito penal. '\\\n",
        "        'Pontue de forma resumida as possíveis penas para um caso de lesão corporal leve sem contexto de violência doméstica.'\n",
        "\n",
        "avaliarTexto(texto)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HO5Ij9_FZTqR"
      },
      "source": [
        "## 6.3 Um astrofísico"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jhvOsGiVZTqZ"
      },
      "outputs": [],
      "source": [
        "def gerar_prompt(texto):\n",
        "\n",
        "    return  f\"\"\"{texto}\\n\n",
        "### Resposta:\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5RdH3yQUZTqa"
      },
      "outputs": [],
      "source": [
        "# Import das bibliotecas\n",
        "from transformers import GenerationConfig\n",
        "import torch\n",
        "import gc\n",
        "\n",
        "def avaliarTexto(texto):\n",
        "    # Recupera o prompt\n",
        "    prompt = gerar_prompt(texto)\n",
        "\n",
        "    # Prepara o prompt para enviar ao modelo realizando sua tokenização\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    # Conecta a entrada prerada ao mesmo dispositivo de computação do modelo\n",
        "    input_ids = inputs[\"input_ids\"].to(model.device)\n",
        "\n",
        "    # Envia a prompt preparado ao modelo\n",
        "    output = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        generation_config=generation_config,\n",
        "        return_dict_in_generate=True,\n",
        "        output_scores=True,\n",
        "        # max_new_tokens=512\n",
        "    )\n",
        "\n",
        "    # Mostra a saída\n",
        "    for s in output.sequences:\n",
        "        # Decodifica a saída\n",
        "        output = tokenizer.decode(s, skip_special_tokens=True)\n",
        "        # Recupera a saída da resposta\n",
        "        saida = output.split(\"### Resposta:\")[1].strip()\n",
        "        print(\"Resposta: \\n\" + saida)\n",
        "\n",
        "    # Liberar memória\n",
        "    del input_ids\n",
        "    del output\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GRoS9dFWZTqa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e1200a3-e49b-408f-d783-f52a316c64ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resposta: \n",
            "Olá! Como astrofísico, podemos entender a expansão do universo através de uma série de evidências e teoremas fundamentais da física.\n",
            "\n",
            "Em primeiro lugar, é importante compreender que o universo é uma sistemática de matéria e energia que se expandiu a partir de uma singularidade infinitamente densa e quente, conhecida como Big Bang. A partir daqui, a matéria e a energia começaram a expandir-se a uma taxa exponencial, o que levou à formação de estrelas, galáxias e outros objetos celestes que observamos hoje em dia.\n",
            "\n",
            "A expansão do universo é governada pela segunda lei de termodinâmica, que estabelece que a entropia de um sistema isolado sempre aumenta com o tempo. A entropia é uma medida da medida da disorganização ou do desordem de um sistema, e a segunda lei de termodinâmica significa que a entropia do universo sempre aumenta, exceto em situações em que há uma transferência de energia ou matéria de um sistema para outro.\n",
            "\n",
            "A expansão do universo também é influenciada pela constante cosmológica, que é uma constante fundamental da física que descreve a taxa de expansão do universo em qualquer momento da história. A constante cosmológica é aproximadamente igual a 67 km/s por megaparsec (Mpc), o que significa que a distância entre dois objetos em movimento aumenta a uma taxa de aproximadamente 67 km/s por cada megaparsec de distância.\n",
            "\n",
            "A evidência para a expansão do universo vem de várias fontes, incluindo a observação de galáxias distantes, a estrutura do cosmos em larga escala e a radiação cósmica de fundo. A observação de galáxias distantes revela que elas estão se movendo em direção ao nosso sistema, o que é consistente com a ideia de que o universo está expandindo-se. A estrutura do cosmos em larga escala, incluindo a distribuição de galáxias e a formação de aglomerados de galáxias, também é consistente com a ideia de que o universo está expandindo-se.\n",
            "\n",
            "Por fim, a radiação cósmica de fundo é uma forma de energia que cobre o universo e que é consistente com a ideia de que o universo está expandindo-se. A radiação cósmica de fundo é uma evidência forte para a expansão do universo, pois sua distribuição é consistente com a ideia de que o universo está expandindo-se a uma taxa constante em todo o tempo.\n",
            "\n",
            "Em resumo, a expansão do universo é uma teoria bem estabelecida na física que é consistente com uma série de evidências e teoremas fundamentais. A expansão do universo é governada pela segunda lei de termodinâmica e pela constante cosmológica, e é consistente com a observação de galáxias distantes, a estrutura do cosmos em larga escala e a radiação cósmica de fundo.\n"
          ]
        }
      ],
      "source": [
        "texto = 'Escreva como se fosse um astrofísico. Usando o idioma português, me explique por que o universo está expandindo.'\n",
        "\n",
        "avaliarTexto(texto)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuClass": "premium",
      "toc_visible": true,
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d26688da76da408c94ee367cd3d15a99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b7db995eeb4c4ec3924cdc62fcd9730a",
              "IPY_MODEL_bd9108bb036a418992a533cc497f5abe",
              "IPY_MODEL_f64dffc5a64246a2832a405f80305bf5"
            ],
            "layout": "IPY_MODEL_e4dc53314bb84e56ac0257f0574e516c"
          }
        },
        "b7db995eeb4c4ec3924cdc62fcd9730a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_79a80962f9fa4b5aaa3ce5f32e2f4b76",
            "placeholder": "​",
            "style": "IPY_MODEL_0125381a28f34a3cb2dca74ddaba53eb",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "bd9108bb036a418992a533cc497f5abe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a18d997034714c28878290cdb73f5ecc",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fe5ca8462cde4ce29e51dff290beb630",
            "value": 2
          }
        },
        "f64dffc5a64246a2832a405f80305bf5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0786b8a01db044008f5db726686651d4",
            "placeholder": "​",
            "style": "IPY_MODEL_44e0f1ab21634065b13641bd3e3c6c49",
            "value": " 2/2 [01:01&lt;00:00, 28.02s/it]"
          }
        },
        "e4dc53314bb84e56ac0257f0574e516c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "79a80962f9fa4b5aaa3ce5f32e2f4b76": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0125381a28f34a3cb2dca74ddaba53eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a18d997034714c28878290cdb73f5ecc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe5ca8462cde4ce29e51dff290beb630": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0786b8a01db044008f5db726686651d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "44e0f1ab21634065b13641bd3e3c6c49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}